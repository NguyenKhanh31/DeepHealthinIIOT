{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import data_read_2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def data_read(s, r):\n",
    "    Label = pd.read_csv(r'Datasets\\Label.csv')\n",
    "\n",
    "    vibr_list = ['Normal', 'Fault-1', 'Fault-2', 'Fault-3', 'Fault-4', 'Fault-5']\n",
    "    vibr_all = {vibr: [] for vibr in vibr_list}\n",
    "\n",
    "    for vibr in vibr_list:\n",
    "        vibr_data = pd.read_csv(f'./Datasets/{vibr}.csv', nrows=r).drop('Index', axis=1).reset_index(drop=True).values.reshape([-1, 4000])\n",
    "        vibr_all[vibr] = [vibr_data[j] for j in range(r)]\n",
    "        del vibr_data\n",
    "        gc.collect()\n",
    "\n",
    "    vibr_all = {k: pd.DataFrame(v).reset_index(drop=True).values.reshape((-1, 1)) for k, v in vibr_all.items()}\n",
    "\n",
    "    X_dataset = pd.concat([pd.DataFrame(vibr_all[vibr]) for vibr in vibr_list], axis=0).reset_index(drop=True).values.reshape([-1, 1])\n",
    "    num_samples = (len(vibr_all['Normal']) // s) * 6\n",
    "    X_dataset = X_dataset[:num_samples * s]\n",
    "    y_dataset = X_dataset[s:num_samples * s]\n",
    "\n",
    "    print(num_samples, len(X_dataset), len(y_dataset))\n",
    "\n",
    "    X_data = X_dataset.reshape((-1, s, 1))\n",
    "    y_data = y_dataset.reshape((-1, s, 1))\n",
    "\n",
    "    label = pd.concat([Label[f\"L{i}\"].iloc[:num_samples // 6] for i in range(6)], axis=0).reset_index(drop=True).values.reshape((-1, 1))\n",
    "\n",
    "    return X_data, y_data, label\n",
    "\n",
    "def data_embedding(x, y, seq_len):\n",
    "    X_data = [pd.DataFrame(x[i]).values.reshape((8, seq_len // 8)) for i in range(len(x))]\n",
    "    y_data = [pd.DataFrame(y[i]).values.reshape((8, seq_len // 8)) for i in range(len(y))]\n",
    "\n",
    "    return X_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tham số\n",
    "seq_length = 1024\n",
    "line_num = 1000\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "hidden_units = seq_length // 8\n",
    "maxlen = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "num_epochs = 300\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "lambda_loss_amount = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- #\n",
    "#  ĐỌC VÀ XỬ LÝ DỮ LIỆU  #\n",
    "# ---------------------- #\n",
    "# Đọc dữ liệu (giả sử data_read_2 có các hàm data_read và data_embedding)\n",
    "X_data, y_data, Label = data_read_2.data_read(seq_length, line_num)\n",
    "X_data, y_data = data_read_2.data_embedding(X_data, y_data, seq_length)\n",
    "print(\"Total data volume: {}\".format(len(X_data)))\n",
    "\n",
    "# Trộn dữ liệu: ghép X_data, y_data và Label lại với nhau rồi random.shuffle\n",
    "Data = list(zip(X_data, y_data, Label))\n",
    "random.shuffle(Data)\n",
    "X_data, y_data, Label = zip(*Data)\n",
    "X_data, y_data, Label = np.array(X_data), np.array(y_data), np.array(Label)\n",
    "\n",
    "# Tách dữ liệu theo tỷ lệ Train 90%, Val 5%, Test 5%\n",
    "num_total = len(X_data)\n",
    "train_end = int(num_total * 0.9) - 1\n",
    "val_end = int(num_total * 0.95) - 1\n",
    "\n",
    "X_train = X_data[:train_end]\n",
    "y_train = y_data[:train_end]\n",
    "\n",
    "X_val = X_data[train_end:val_end]\n",
    "y_val = y_data[train_end:val_end]\n",
    "\n",
    "X_test = X_data[val_end:num_total-1]\n",
    "y_test = y_data[val_end:num_total-1]\n",
    "Label_test = Label[val_end:num_total-1]\n",
    "\n",
    "print(\"Train data volume: {}, Val data volume: {}, Test data volume: {}\"\n",
    "      .format(len(X_train), len(X_val), len(X_test)))\n",
    "\n",
    "# ---------------------- #\n",
    "#  CHUẨN BỊ DỮ LIỆU    #\n",
    "# ---------------------- #\n",
    "# Ép kiểu và chuyển đổi dữ liệu sang numpy array kiểu float32\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "X_val   = np.array(X_val, dtype=np.float32)\n",
    "y_val   = np.array(y_val, dtype=np.float32)\n",
    "X_test  = np.array(X_test, dtype=np.float32)\n",
    "y_test  = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "# Tạo tf.data.Dataset cho train, validation và test\n",
    "train_inputs = (X_train, y_train)\n",
    "train_targets = y_train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(batch_size)\n",
    "\n",
    "val_inputs = (X_val, y_val)\n",
    "val_targets = y_val\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_targets)).batch(batch_size)\n",
    "\n",
    "test_inputs = (X_test, y_test)\n",
    "test_targets = y_test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_targets)).batch(32)\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#    ĐỊNH NGHĨA MODEL   #\n",
    "# ---------------------- #\n",
    "\n",
    "# MultiHeadAttention custom layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate, causality=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.linear_Q = tf.keras.layers.Dense(d_model)\n",
    "        self.linear_K = tf.keras.layers.Dense(d_model)\n",
    "        self.linear_V = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.causality = causality\n",
    "\n",
    "    def call(self, queries, keys, values, training):\n",
    "        # queries, keys, values: shape (batch, seq_len, d_model)\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        Q = self.linear_Q(queries)  # (batch, seq_len, d_model)\n",
    "        K = self.linear_K(keys)\n",
    "        V = self.linear_V(values)\n",
    "        \n",
    "        # Tách ra theo số head: chuyển về shape (batch, num_heads, seq_len, d_k)\n",
    "        def split_heads(x):\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "        \n",
    "        # Tính attention scores với scaled dot-product\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_k, tf.float32))\n",
    "        if self.causality:\n",
    "            # Tạo mask causal: chỉ cho phép truy cập thông tin quá khứ\n",
    "            seq_len = tf.shape(scores)[-1]\n",
    "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            mask = tf.cast(mask, tf.bool)\n",
    "            scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        attn = self.dropout(attn, training=training)\n",
    "        output = tf.matmul(attn, V)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Nối lại các head\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch, seq_len, num_heads, d_k)\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Kết hợp residual connection và layer normalization\n",
    "        output = self.layer_norm(output + queries)\n",
    "        return output\n",
    "\n",
    "# FeedForward layer\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(4 * d_model)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        residual = x\n",
    "        x = self.linear1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x\n",
    "\n",
    "# Encoder block\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=False)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        x = self.mha(x, x, x, training=training)\n",
    "        x = self.ffn(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Decoder block\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=True)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=False)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training):\n",
    "        x = self.mha1(x, x, x, training=training)\n",
    "        x = self.mha2(x, enc_output, enc_output, training=training)\n",
    "        x = self.ffn(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Linear layer cuối cùng\n",
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        self.output_scale = self.add_weight(name=\"output_scale\", shape=(), initializer=tf.keras.initializers.Ones())\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.linear(x) * self.output_scale\n",
    "\n",
    "# Mô hình Transformer chính\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_rate, lambda_loss_amount):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layers = [EncoderBlock(d_model, num_heads, dropout_rate) for _ in range(num_encoder_layers)]\n",
    "        self.decoder_layers = [DecoderBlock(d_model, num_heads, dropout_rate) for _ in range(num_decoder_layers)]\n",
    "        self.linear_layer = LinearLayer(d_model)\n",
    "        self.lambda_loss_amount = lambda_loss_amount\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs là một tuple chứa (encoder_input, decoder_input)\n",
    "        encoder_input, decoder_input = inputs\n",
    "        for enc in self.encoder_layers:\n",
    "            encoder_input = enc(encoder_input, training=training)\n",
    "        enc_output = encoder_input\n",
    "        for dec in self.decoder_layers:\n",
    "            decoder_input = dec(decoder_input, enc_output, training=training)\n",
    "        pred = self.linear_layer(decoder_input)\n",
    "        return pred\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#    KHỞI TẠO MODEL    #\n",
    "# ---------------------- #\n",
    "model = TransformerModel(d_model=hidden_units, num_heads=num_heads,\n",
    "                         num_encoder_layers=num_encoder_layers,\n",
    "                         num_decoder_layers=num_decoder_layers,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                         lambda_loss_amount=lambda_loss_amount)\n",
    "\n",
    "# Biên dịch model với optimizer và loss (MSE)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# ---------------------- #\n",
    "#       HUẤN LUYỆN      #\n",
    "# ---------------------- #\n",
    "print(\"Training...\\n\")\n",
    "start_time = time.time()\n",
    "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=2)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Lưu model (chỉ lưu weights)\n",
    "model_path = f'./DH2-{seq_length}.weights.h5'\n",
    "model.save_weights(model_path)\n",
    "print(f\"Model saved as {model_path}\")\n",
    "\n",
    "# Đánh giá trên tập validation\n",
    "val_loss = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Tải lại model đã lưu để demo\n",
    "model_loaded = TransformerModel(d_model=hidden_units, num_heads=num_heads,\n",
    "                                num_encoder_layers=num_encoder_layers,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                lambda_loss_amount=lambda_loss_amount)\n",
    "# Đảm bảo xây dựng model bằng cách chạy một batch mẫu:\n",
    "dummy_encoder = tf.random.uniform((1, X_train.shape[1], hidden_units))\n",
    "dummy_decoder = tf.random.uniform((1, y_train.shape[1], hidden_units))\n",
    "_ = model_loaded((dummy_encoder, dummy_decoder))\n",
    "model_loaded.load_weights(model_path)\n",
    "\n",
    "# ---------------------- #\n",
    "#         KIỂM TRA      #\n",
    "# ---------------------- #\n",
    "test_time_start = time.time()\n",
    "all_preds = []\n",
    "for (enc_input, dec_input), target in test_dataset:\n",
    "    preds = model_loaded((enc_input, dec_input), training=False)\n",
    "    all_preds.append(preds)\n",
    "all_preds = tf.concat(all_preds, axis=0)\n",
    "test_time_end = time.time()\n",
    "test_time = test_time_end - test_time_start\n",
    "\n",
    "\n",
    "# Tính RMSE và MAE\n",
    "mse = tf.keras.losses.MeanSquaredError()(y_test, all_preds)\n",
    "rmse = tf.sqrt(mse).numpy()\n",
    "mae = tf.keras.losses.MeanAbsoluteError()(y_test, all_preds).numpy()\n",
    "\n",
    "print(f\"The final RMSE = {rmse:.4f}, The final MAE = {mae:.4f}\")\n",
    "print(f\"Test time: {test_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
