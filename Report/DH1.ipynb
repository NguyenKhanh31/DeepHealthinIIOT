{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import data_read\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Conv1D, Flatten\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "def data_read(s, r):\n",
    "    Label = pd.read_csv('Datasets/Label.csv')\n",
    "\n",
    "    vibr_list = ['Normal', 'Fault-1', 'Fault-2', 'Fault-3', 'Fault-4', 'Fault-5']\n",
    "    vibr_all = {vibr: [] for vibr in vibr_list}\n",
    "\n",
    "    for vibr in vibr_list:\n",
    "        vibr_data = pd.read_csv(f'Datasets/{vibr}.csv', nrows=r).drop('Index', axis=1).reset_index(drop=True).values.reshape([-1, 4000])\n",
    "        vibr_all[vibr] = [vibr_data[j] for j in range(r)]\n",
    "\n",
    "        del vibr_data\n",
    "        gc.collect()\n",
    "\n",
    "    for key in vibr_all:\n",
    "        vibr_all[key] = pd.DataFrame(vibr_all[key]).reset_index(drop=True).values.reshape((-1, 1))\n",
    "\n",
    "    X_dataset = pd.concat([pd.DataFrame(vibr_all[vibr]) for vibr in vibr_list], axis=0).reset_index(drop=True).values.reshape([-1, 1])\n",
    "\n",
    "    num_samples = (len(vibr_all['Normal']) // s) * 6\n",
    "    X_data = pd.concat([pd.DataFrame(X_dataset[:num_samples * s])], axis=1).reset_index(drop=True).values.reshape((-1, s, 1))\n",
    "\n",
    "    y_data = pd.concat([Label[f\"L{i}\"].iloc[:num_samples // 6] for i in range(6)], axis=0).reset_index(drop=True).values.reshape((-1, 1))\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "def data_embedding(x, seq_len):\n",
    "    X_data = []\n",
    "    for i in range(0, len(x)):\n",
    "        x_data = x[i]\n",
    "        x_data = pd.DataFrame(x_data).values.reshape((8, int(seq_len / 8)))\n",
    "        X_data.append(x_data)\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1024\n",
    "line_num = 1000\n",
    "\n",
    "# Data\n",
    "X_data, y_data = data_read.data_read(seq_length, line_num)\n",
    "X_data = data_embedding.data_embedding(X_data, seq_length)\n",
    "print(\"Total data volume: {}\".format(len(X_data)))\n",
    "\n",
    "# Shuffle\n",
    "Data = list(zip(X_data, y_data))\n",
    "random.shuffle(Data)\n",
    "X_data, y_data = zip(*Data)\n",
    "X_data, y_data = np.array(X_data), np.array(y_data)\n",
    "\n",
    "# Data split\n",
    "X_train, y_train = X_data[0:int(len(X_data)*0.7)-1], y_data[0:int(len(y_data)*0.7)-1]\n",
    "X_valuate, y_valuate = X_data[int(len(X_data)*0.7):int(len(X_data)*0.9)-1], y_data[int(len(X_data)*0.7):int(len(X_data)*0.9)-1]\n",
    "X_test, y_test = X_data[int(len(X_data)*0.9):len(X_data)-1], y_data[int(len(X_data)*0.9):len(y_data)-1]\n",
    "print(\"Train data volume: {}\".format(len(X_train)), \"Valuate data volume: {}\".format(len(X_valuate)), \"Teat data volume: {}\".format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "hidden_units = seq_length // 8\n",
    "maxlen = 8\n",
    "num_blocks = 3\n",
    "num_epochs = 300\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "class NormalizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.layer_norm(inputs)\n",
    "\n",
    "class MultiheadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, num_heads=num_heads, dropout_rate=dropout_rate):\n",
    "        super(MultiheadAttentionLayer, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense_Q = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.dense_K = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.dense_V = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.normalize = NormalizeLayer()\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        return tf.concat(tf.split(x, self.num_heads, axis=-1), axis=0)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        return tf.concat(tf.split(x, self.num_heads, axis=0), axis=-1)\n",
    "\n",
    "    def call(self, queries, keys):\n",
    "        Q = self.dense_Q(queries)\n",
    "        K = self.dense_K(keys)\n",
    "        V = self.dense_V(keys)\n",
    "\n",
    "        Q_ = self.split_heads(Q)\n",
    "        K_ = self.split_heads(K)\n",
    "        V_ = self.split_heads(V)\n",
    "\n",
    "        scores = tf.matmul(Q_, K_, transpose_b=True) / tf.math.sqrt(tf.cast(K_.shape[-1], tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores)\n",
    "\n",
    "        query_masks = tf.cast(tf.reduce_sum(tf.abs(queries), axis=-1, keepdims=True) > 0, tf.float32)\n",
    "        query_masks = tf.tile(query_masks, [self.num_heads, 1, tf.shape(keys)[1]])\n",
    "        attention_weights *= query_masks\n",
    "\n",
    "        outputs = tf.matmul(attention_weights, V_)\n",
    "        outputs = self.merge_heads(outputs)\n",
    "        outputs += queries\n",
    "\n",
    "        return self.normalize(outputs)\n",
    "\n",
    "class FeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=num_units[0], kernel_size=1, activation=tf.nn.relu, use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=num_units[1], kernel_size=1, activation=None, use_bias=True)\n",
    "        self.normalize = NormalizeLayer()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs += inputs\n",
    "        return self.normalize(outputs)\n",
    "\n",
    "def one_hot_encoding(y_):\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    y_ = y_.reshape(-1, 1)\n",
    "    return encoder.fit_transform(y_)\n",
    "\n",
    "class MultiheadAttentionLayerWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, num_heads=num_heads, dropout_rate=dropout_rate):\n",
    "        super(MultiheadAttentionLayerWrapper, self).__init__()\n",
    "        self.multihead_attention = MultiheadAttentionLayer(num_units, num_heads, dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        queries, keys = inputs\n",
    "        return self.multihead_attention(queries, keys)\n",
    "\n",
    "# Define model\n",
    "def build_model():\n",
    "    inputs = tf.keras.Input(shape=(maxlen, hidden_units))\n",
    "    enc = inputs\n",
    "    for _ in range(num_blocks):\n",
    "        enc = MultiheadAttentionLayerWrapper(hidden_units)([enc, enc])\n",
    "    outputs = tf.keras.layers.Dense(6, activation='softmax')(tf.keras.layers.Flatten()(enc))\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = build_model()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Use existing data\n",
    "y_train = one_hot_encoding(y_train)\n",
    "y_val = one_hot_encoding(y_valuate)\n",
    "y_test = one_hot_encoding(y_test)\n",
    "\n",
    "# Define X_val\n",
    "X_val = X_valuate\n",
    "\n",
    "# Train the model\n",
    "time_start = time.time()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epochs)\n",
    "time_end = time.time()\n",
    "train_time = time_end - time_start\n",
    "print(f\"Training time: {train_time:.3f}s\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_time_start = time.time()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "test_time_end = time.time()\n",
    "test_time = test_time_end - test_time_start\n",
    "print(f\"Test Accuracy: {test_acc:.5f}, Test Time: {test_time:.3f}s, Train Time: {train_time:.3f}s\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dh1_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
