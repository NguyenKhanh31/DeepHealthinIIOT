{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data volume: 23436\n",
      "Train data volume: 16404 Valuate data volume: 4686 Teat data volume: 2343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import data_embedding, data_read, modules_test\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Conv1D, Flatten\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data volume: 23436\n",
      "Train data volume: 16404 Valuate data volume: 4686 Teat data volume: 2343\n"
     ]
    }
   ],
   "source": [
    "seq_length = 1024\n",
    "line_num = 1000\n",
    "\n",
    "# Data\n",
    "X_data, y_data = data_read.data_read(seq_length, line_num)\n",
    "X_data = data_embedding.data_embedding(X_data, seq_length)\n",
    "print(\"Total data volume: {}\".format(len(X_data)))\n",
    "\n",
    "# Shuffle\n",
    "Data = list(zip(X_data, y_data))\n",
    "random.shuffle(Data)\n",
    "X_data, y_data = zip(*Data)\n",
    "X_data, y_data = np.array(X_data), np.array(y_data)\n",
    "\n",
    "# Data split\n",
    "X_train, y_train = X_data[0:int(len(X_data)*0.7)-1], y_data[0:int(len(y_data)*0.7)-1]\n",
    "X_valuate, y_valuate = X_data[int(len(X_data)*0.7):int(len(X_data)*0.9)-1], y_data[int(len(X_data)*0.7):int(len(X_data)*0.9)-1]\n",
    "X_test, y_test = X_data[int(len(X_data)*0.9):len(X_data)-1], y_data[int(len(X_data)*0.9):len(y_data)-1]\n",
    "print(\"Train data volume: {}\".format(len(X_train)), \"Valuate data volume: {}\".format(len(X_valuate)), \"Teat data volume: {}\".format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Hi Windows 11 Home\\anaconda3\\envs\\SOTA\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - accuracy: 0.2049 - loss: 2.1160 - val_accuracy: 0.3020 - val_loss: 1.7460\n",
      "Epoch 2/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.3413 - loss: 1.6548 - val_accuracy: 0.4539 - val_loss: 1.4288\n",
      "Epoch 3/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.4802 - loss: 1.3351 - val_accuracy: 0.5717 - val_loss: 1.1358\n",
      "Epoch 4/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.5888 - loss: 1.0731 - val_accuracy: 0.6466 - val_loss: 0.8983\n",
      "Epoch 5/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.6757 - loss: 0.8518 - val_accuracy: 0.7172 - val_loss: 0.7289\n",
      "Epoch 6/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.7475 - loss: 0.6776 - val_accuracy: 0.7682 - val_loss: 0.6070\n",
      "Epoch 7/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.7975 - loss: 0.5587 - val_accuracy: 0.8124 - val_loss: 0.5137\n",
      "Epoch 8/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.8412 - loss: 0.4637 - val_accuracy: 0.8487 - val_loss: 0.4358\n",
      "Epoch 9/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.8671 - loss: 0.3950 - val_accuracy: 0.8726 - val_loss: 0.3771\n",
      "Epoch 10/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9043 - loss: 0.3143 - val_accuracy: 0.8886 - val_loss: 0.3256\n",
      "Epoch 11/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9155 - loss: 0.2819 - val_accuracy: 0.8993 - val_loss: 0.2913\n",
      "Epoch 12/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.9254 - loss: 0.2392 - val_accuracy: 0.9153 - val_loss: 0.2550\n",
      "Epoch 13/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.9386 - loss: 0.2053 - val_accuracy: 0.9210 - val_loss: 0.2322\n",
      "Epoch 14/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.9484 - loss: 0.1845 - val_accuracy: 0.9270 - val_loss: 0.2142\n",
      "Epoch 15/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9521 - loss: 0.1620 - val_accuracy: 0.9341 - val_loss: 0.1972\n",
      "Epoch 16/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9608 - loss: 0.1447 - val_accuracy: 0.9366 - val_loss: 0.1893\n",
      "Epoch 17/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9672 - loss: 0.1271 - val_accuracy: 0.9402 - val_loss: 0.1776\n",
      "Epoch 18/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9703 - loss: 0.1130 - val_accuracy: 0.9383 - val_loss: 0.1721\n",
      "Epoch 19/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9756 - loss: 0.0997 - val_accuracy: 0.9417 - val_loss: 0.1616\n",
      "Epoch 20/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.9760 - loss: 0.0939 - val_accuracy: 0.9486 - val_loss: 0.1534\n",
      "Epoch 21/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 60ms/step - accuracy: 0.9817 - loss: 0.0806 - val_accuracy: 0.9511 - val_loss: 0.1469\n",
      "Epoch 22/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9834 - loss: 0.0760 - val_accuracy: 0.9511 - val_loss: 0.1422\n",
      "Epoch 23/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - accuracy: 0.9872 - loss: 0.0626 - val_accuracy: 0.9524 - val_loss: 0.1397\n",
      "Epoch 24/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9872 - loss: 0.0593 - val_accuracy: 0.9507 - val_loss: 0.1369\n",
      "Epoch 25/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9899 - loss: 0.0519 - val_accuracy: 0.9541 - val_loss: 0.1316\n",
      "Epoch 26/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9902 - loss: 0.0515 - val_accuracy: 0.9556 - val_loss: 0.1283\n",
      "Epoch 27/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9939 - loss: 0.0381 - val_accuracy: 0.9539 - val_loss: 0.1274\n",
      "Epoch 28/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9958 - loss: 0.0358 - val_accuracy: 0.9595 - val_loss: 0.1224\n",
      "Epoch 29/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9964 - loss: 0.0305 - val_accuracy: 0.9573 - val_loss: 0.1221\n",
      "Epoch 30/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.9972 - loss: 0.0277 - val_accuracy: 0.9590 - val_loss: 0.1233\n",
      "Epoch 31/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.9980 - loss: 0.0219 - val_accuracy: 0.9571 - val_loss: 0.1205\n",
      "Epoch 32/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.9983 - loss: 0.0208 - val_accuracy: 0.9586 - val_loss: 0.1194\n",
      "Epoch 33/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9989 - loss: 0.0189 - val_accuracy: 0.9614 - val_loss: 0.1158\n",
      "Epoch 34/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9996 - loss: 0.0152 - val_accuracy: 0.9629 - val_loss: 0.1150\n",
      "Epoch 35/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9993 - loss: 0.0133 - val_accuracy: 0.9616 - val_loss: 0.1174\n",
      "Epoch 36/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9994 - loss: 0.0127 - val_accuracy: 0.9603 - val_loss: 0.1200\n",
      "Epoch 37/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9999 - loss: 0.0101 - val_accuracy: 0.9644 - val_loss: 0.1132\n",
      "Epoch 38/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9999 - loss: 0.0085 - val_accuracy: 0.9627 - val_loss: 0.1140\n",
      "Epoch 39/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9644 - val_loss: 0.1154\n",
      "Epoch 40/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.9637 - val_loss: 0.1152\n",
      "Epoch 41/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9652 - val_loss: 0.1137\n",
      "Epoch 42/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9661 - val_loss: 0.1140\n",
      "Epoch 43/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.9654 - val_loss: 0.1149\n",
      "Epoch 44/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.9665 - val_loss: 0.1145\n",
      "Epoch 45/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9661 - val_loss: 0.1170\n",
      "Epoch 46/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.9671 - val_loss: 0.1142\n",
      "Epoch 47/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9669 - val_loss: 0.1127\n",
      "Epoch 48/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9671 - val_loss: 0.1136\n",
      "Epoch 49/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9673 - val_loss: 0.1145\n",
      "Epoch 50/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9671 - val_loss: 0.1157\n",
      "Epoch 51/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9663 - val_loss: 0.1178\n",
      "Epoch 52/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9682 - val_loss: 0.1168\n",
      "Epoch 53/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9682 - val_loss: 0.1163\n",
      "Epoch 54/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9686 - val_loss: 0.1172\n",
      "Epoch 55/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9682 - val_loss: 0.1190\n",
      "Epoch 56/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9688 - val_loss: 0.1183\n",
      "Epoch 57/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 9.8621e-04 - val_accuracy: 0.9676 - val_loss: 0.1193\n",
      "Epoch 58/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 9.0989e-04 - val_accuracy: 0.9682 - val_loss: 0.1195\n",
      "Epoch 59/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 8.0953e-04 - val_accuracy: 0.9678 - val_loss: 0.1196\n",
      "Epoch 60/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 7.3711e-04 - val_accuracy: 0.9682 - val_loss: 0.1198\n",
      "Epoch 61/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 6.9851e-04 - val_accuracy: 0.9676 - val_loss: 0.1216\n",
      "Epoch 62/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 6.3379e-04 - val_accuracy: 0.9684 - val_loss: 0.1206\n",
      "Epoch 63/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.6509e-04 - val_accuracy: 0.9686 - val_loss: 0.1226\n",
      "Epoch 64/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 5.1885e-04 - val_accuracy: 0.9693 - val_loss: 0.1204\n",
      "Epoch 65/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 5.0453e-04 - val_accuracy: 0.9686 - val_loss: 0.1231\n",
      "Epoch 66/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 4.3832e-04 - val_accuracy: 0.9693 - val_loss: 0.1228\n",
      "Epoch 67/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 4.0460e-04 - val_accuracy: 0.9688 - val_loss: 0.1231\n",
      "Epoch 68/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.7263e-04 - val_accuracy: 0.9682 - val_loss: 0.1242\n",
      "Epoch 69/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.3503e-04 - val_accuracy: 0.9682 - val_loss: 0.1240\n",
      "Epoch 70/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.1580e-04 - val_accuracy: 0.9682 - val_loss: 0.1262\n",
      "Epoch 71/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.8482e-04 - val_accuracy: 0.9691 - val_loss: 0.1268\n",
      "Epoch 72/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 2.6673e-04 - val_accuracy: 0.9691 - val_loss: 0.1257\n",
      "Epoch 73/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 2.4145e-04 - val_accuracy: 0.9688 - val_loss: 0.1271\n",
      "Epoch 74/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 2.2563e-04 - val_accuracy: 0.9697 - val_loss: 0.1265\n",
      "Epoch 75/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.0814e-04 - val_accuracy: 0.9691 - val_loss: 0.1271\n",
      "Epoch 76/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.8630e-04 - val_accuracy: 0.9693 - val_loss: 0.1274\n",
      "Epoch 77/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.7317e-04 - val_accuracy: 0.9693 - val_loss: 0.1289\n",
      "Epoch 78/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.6436e-04 - val_accuracy: 0.9693 - val_loss: 0.1305\n",
      "Epoch 79/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 1.4457e-04 - val_accuracy: 0.9691 - val_loss: 0.1289\n",
      "Epoch 80/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.3552e-04 - val_accuracy: 0.9693 - val_loss: 0.1299\n",
      "Epoch 81/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.2833e-04 - val_accuracy: 0.9701 - val_loss: 0.1306\n",
      "Epoch 82/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.1540e-04 - val_accuracy: 0.9691 - val_loss: 0.1314\n",
      "Epoch 83/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.0543e-04 - val_accuracy: 0.9701 - val_loss: 0.1320\n",
      "Epoch 84/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 9.5258e-05 - val_accuracy: 0.9693 - val_loss: 0.1326\n",
      "Epoch 85/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 8.7671e-05 - val_accuracy: 0.9706 - val_loss: 0.1321\n",
      "Epoch 86/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 8.4444e-05 - val_accuracy: 0.9686 - val_loss: 0.1347\n",
      "Epoch 87/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 7.6876e-05 - val_accuracy: 0.9697 - val_loss: 0.1345\n",
      "Epoch 88/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 6.9071e-05 - val_accuracy: 0.9701 - val_loss: 0.1348\n",
      "Epoch 89/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 6.3968e-05 - val_accuracy: 0.9680 - val_loss: 0.1367\n",
      "Epoch 90/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 5.9137e-05 - val_accuracy: 0.9697 - val_loss: 0.1356\n",
      "Epoch 91/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 5.5405e-05 - val_accuracy: 0.9699 - val_loss: 0.1372\n",
      "Epoch 92/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 5.1026e-05 - val_accuracy: 0.9697 - val_loss: 0.1373\n",
      "Epoch 93/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 4.7843e-05 - val_accuracy: 0.9697 - val_loss: 0.1374\n",
      "Epoch 94/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 4.5555e-05 - val_accuracy: 0.9708 - val_loss: 0.1369\n",
      "Epoch 95/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 4.1292e-05 - val_accuracy: 0.9691 - val_loss: 0.1409\n",
      "Epoch 96/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.8439e-05 - val_accuracy: 0.9699 - val_loss: 0.1388\n",
      "Epoch 97/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 3.2616e-05 - val_accuracy: 0.9701 - val_loss: 0.1396\n",
      "Epoch 98/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.1694e-05 - val_accuracy: 0.9701 - val_loss: 0.1398\n",
      "Epoch 99/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 2.9611e-05 - val_accuracy: 0.9701 - val_loss: 0.1408\n",
      "Epoch 100/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 2.7614e-05 - val_accuracy: 0.9701 - val_loss: 0.1413\n",
      "Epoch 101/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 2.5110e-05 - val_accuracy: 0.9699 - val_loss: 0.1419\n",
      "Epoch 102/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.2996e-05 - val_accuracy: 0.9699 - val_loss: 0.1431\n",
      "Epoch 103/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.1523e-05 - val_accuracy: 0.9697 - val_loss: 0.1436\n",
      "Epoch 104/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 2.0167e-05 - val_accuracy: 0.9699 - val_loss: 0.1436\n",
      "Epoch 105/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.8824e-05 - val_accuracy: 0.9701 - val_loss: 0.1440\n",
      "Epoch 106/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.7035e-05 - val_accuracy: 0.9699 - val_loss: 0.1457\n",
      "Epoch 107/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.6362e-05 - val_accuracy: 0.9706 - val_loss: 0.1458\n",
      "Epoch 108/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 1.3945e-05 - val_accuracy: 0.9701 - val_loss: 0.1463\n",
      "Epoch 109/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.3639e-05 - val_accuracy: 0.9703 - val_loss: 0.1464\n",
      "Epoch 110/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.2627e-05 - val_accuracy: 0.9699 - val_loss: 0.1462\n",
      "Epoch 111/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 1.1433e-05 - val_accuracy: 0.9701 - val_loss: 0.1471\n",
      "Epoch 112/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.0797e-05 - val_accuracy: 0.9703 - val_loss: 0.1469\n",
      "Epoch 113/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 9.9595e-06 - val_accuracy: 0.9706 - val_loss: 0.1499\n",
      "Epoch 114/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 9.4318e-06 - val_accuracy: 0.9706 - val_loss: 0.1489\n",
      "Epoch 115/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 8.8925e-06 - val_accuracy: 0.9703 - val_loss: 0.1503\n",
      "Epoch 116/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 7.7680e-06 - val_accuracy: 0.9703 - val_loss: 0.1499\n",
      "Epoch 117/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 7.2749e-06 - val_accuracy: 0.9703 - val_loss: 0.1508\n",
      "Epoch 118/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 6.9672e-06 - val_accuracy: 0.9701 - val_loss: 0.1505\n",
      "Epoch 119/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 6.5332e-06 - val_accuracy: 0.9706 - val_loss: 0.1523\n",
      "Epoch 120/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 6.0926e-06 - val_accuracy: 0.9703 - val_loss: 0.1524\n",
      "Epoch 121/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 5.5170e-06 - val_accuracy: 0.9710 - val_loss: 0.1517\n",
      "Epoch 122/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.0582e-06 - val_accuracy: 0.9706 - val_loss: 0.1538\n",
      "Epoch 123/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 4.5663e-06 - val_accuracy: 0.9708 - val_loss: 0.1531\n",
      "Epoch 124/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 4.6450e-06 - val_accuracy: 0.9708 - val_loss: 0.1547\n",
      "Epoch 125/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 4.0310e-06 - val_accuracy: 0.9706 - val_loss: 0.1539\n",
      "Epoch 126/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 3.8209e-06 - val_accuracy: 0.9712 - val_loss: 0.1557\n",
      "Epoch 127/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 3.5298e-06 - val_accuracy: 0.9710 - val_loss: 0.1563\n",
      "Epoch 128/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 3.2140e-06 - val_accuracy: 0.9710 - val_loss: 0.1567\n",
      "Epoch 129/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 3.0839e-06 - val_accuracy: 0.9703 - val_loss: 0.1582\n",
      "Epoch 130/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.8013e-06 - val_accuracy: 0.9708 - val_loss: 0.1569\n",
      "Epoch 131/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.7469e-06 - val_accuracy: 0.9708 - val_loss: 0.1582\n",
      "Epoch 132/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.4252e-06 - val_accuracy: 0.9706 - val_loss: 0.1587\n",
      "Epoch 133/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 2.3296e-06 - val_accuracy: 0.9710 - val_loss: 0.1607\n",
      "Epoch 134/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 2.1330e-06 - val_accuracy: 0.9708 - val_loss: 0.1594\n",
      "Epoch 135/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 1.9152e-06 - val_accuracy: 0.9710 - val_loss: 0.1605\n",
      "Epoch 136/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.7861e-06 - val_accuracy: 0.9716 - val_loss: 0.1608\n",
      "Epoch 137/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.6661e-06 - val_accuracy: 0.9714 - val_loss: 0.1599\n",
      "Epoch 138/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.5713e-06 - val_accuracy: 0.9714 - val_loss: 0.1618\n",
      "Epoch 139/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.4675e-06 - val_accuracy: 0.9714 - val_loss: 0.1629\n",
      "Epoch 140/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.3301e-06 - val_accuracy: 0.9712 - val_loss: 0.1640\n",
      "Epoch 141/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 1.3262e-06 - val_accuracy: 0.9710 - val_loss: 0.1628\n",
      "Epoch 142/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.1685e-06 - val_accuracy: 0.9708 - val_loss: 0.1647\n",
      "Epoch 143/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.0869e-06 - val_accuracy: 0.9710 - val_loss: 0.1641\n",
      "Epoch 144/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.0239e-06 - val_accuracy: 0.9712 - val_loss: 0.1656\n",
      "Epoch 145/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 9.7315e-07 - val_accuracy: 0.9710 - val_loss: 0.1652\n",
      "Epoch 146/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 8.5806e-07 - val_accuracy: 0.9712 - val_loss: 0.1647\n",
      "Epoch 147/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 8.2336e-07 - val_accuracy: 0.9714 - val_loss: 0.1660\n",
      "Epoch 148/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 7.5816e-07 - val_accuracy: 0.9712 - val_loss: 0.1662\n",
      "Epoch 149/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 7.0875e-07 - val_accuracy: 0.9712 - val_loss: 0.1673\n",
      "Epoch 150/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 6.3245e-07 - val_accuracy: 0.9712 - val_loss: 0.1672\n",
      "Epoch 151/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 6.2938e-07 - val_accuracy: 0.9712 - val_loss: 0.1677\n",
      "Epoch 152/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 5.8495e-07 - val_accuracy: 0.9714 - val_loss: 0.1682\n",
      "Epoch 153/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.6291e-07 - val_accuracy: 0.9714 - val_loss: 0.1699\n",
      "Epoch 154/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 4.8308e-07 - val_accuracy: 0.9716 - val_loss: 0.1693\n",
      "Epoch 155/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 4.6316e-07 - val_accuracy: 0.9714 - val_loss: 0.1693\n",
      "Epoch 156/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 4.4022e-07 - val_accuracy: 0.9716 - val_loss: 0.1719\n",
      "Epoch 157/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.8911e-07 - val_accuracy: 0.9710 - val_loss: 0.1717\n",
      "Epoch 158/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.7711e-07 - val_accuracy: 0.9712 - val_loss: 0.1715\n",
      "Epoch 159/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.5623e-07 - val_accuracy: 0.9712 - val_loss: 0.1726\n",
      "Epoch 160/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 3.3651e-07 - val_accuracy: 0.9712 - val_loss: 0.1741\n",
      "Epoch 161/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.0768e-07 - val_accuracy: 0.9716 - val_loss: 0.1728\n",
      "Epoch 162/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.9301e-07 - val_accuracy: 0.9710 - val_loss: 0.1745\n",
      "Epoch 163/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.6914e-07 - val_accuracy: 0.9716 - val_loss: 0.1746\n",
      "Epoch 164/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.4979e-07 - val_accuracy: 0.9716 - val_loss: 0.1760\n",
      "Epoch 165/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.3607e-07 - val_accuracy: 0.9712 - val_loss: 0.1753\n",
      "Epoch 166/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.1675e-07 - val_accuracy: 0.9714 - val_loss: 0.1758\n",
      "Epoch 167/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.0792e-07 - val_accuracy: 0.9720 - val_loss: 0.1755\n",
      "Epoch 168/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.8727e-07 - val_accuracy: 0.9718 - val_loss: 0.1773\n",
      "Epoch 169/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.8006e-07 - val_accuracy: 0.9712 - val_loss: 0.1777\n",
      "Epoch 170/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.6989e-07 - val_accuracy: 0.9718 - val_loss: 0.1780\n",
      "Epoch 171/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.6399e-07 - val_accuracy: 0.9723 - val_loss: 0.1780\n",
      "Epoch 172/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.5096e-07 - val_accuracy: 0.9718 - val_loss: 0.1782\n",
      "Epoch 173/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.4212e-07 - val_accuracy: 0.9720 - val_loss: 0.1787\n",
      "Epoch 174/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 1.3200e-07 - val_accuracy: 0.9720 - val_loss: 0.1786\n",
      "Epoch 175/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.2279e-07 - val_accuracy: 0.9718 - val_loss: 0.1810\n",
      "Epoch 176/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.2060e-07 - val_accuracy: 0.9723 - val_loss: 0.1815\n",
      "Epoch 177/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 1.0922e-07 - val_accuracy: 0.9723 - val_loss: 0.1815\n",
      "Epoch 178/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 1.0425e-07 - val_accuracy: 0.9720 - val_loss: 0.1805\n",
      "Epoch 179/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 9.4564e-08 - val_accuracy: 0.9727 - val_loss: 0.1830\n",
      "Epoch 180/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 9.2485e-08 - val_accuracy: 0.9723 - val_loss: 0.1814\n",
      "Epoch 181/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 8.5671e-08 - val_accuracy: 0.9718 - val_loss: 0.1830\n",
      "Epoch 182/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 8.1960e-08 - val_accuracy: 0.9720 - val_loss: 0.1823\n",
      "Epoch 183/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 7.6814e-08 - val_accuracy: 0.9720 - val_loss: 0.1828\n",
      "Epoch 184/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 7.0983e-08 - val_accuracy: 0.9718 - val_loss: 0.1850\n",
      "Epoch 185/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 6.5595e-08 - val_accuracy: 0.9720 - val_loss: 0.1832\n",
      "Epoch 186/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 6.3385e-08 - val_accuracy: 0.9725 - val_loss: 0.1854\n",
      "Epoch 187/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 5.8408e-08 - val_accuracy: 0.9727 - val_loss: 0.1848\n",
      "Epoch 188/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.7655e-08 - val_accuracy: 0.9720 - val_loss: 0.1849\n",
      "Epoch 189/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.4764e-08 - val_accuracy: 0.9725 - val_loss: 0.1859\n",
      "Epoch 190/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.1857e-08 - val_accuracy: 0.9731 - val_loss: 0.1875\n",
      "Epoch 191/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 4.6874e-08 - val_accuracy: 0.9725 - val_loss: 0.1865\n",
      "Epoch 192/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 4.6161e-08 - val_accuracy: 0.9725 - val_loss: 0.1880\n",
      "Epoch 193/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 4.2085e-08 - val_accuracy: 0.9723 - val_loss: 0.1874\n",
      "Epoch 194/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 4.0630e-08 - val_accuracy: 0.9725 - val_loss: 0.1880\n",
      "Epoch 195/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.9039e-08 - val_accuracy: 0.9729 - val_loss: 0.1895\n",
      "Epoch 196/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.6527e-08 - val_accuracy: 0.9729 - val_loss: 0.1898\n",
      "Epoch 197/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.4427e-08 - val_accuracy: 0.9725 - val_loss: 0.1886\n",
      "Epoch 198/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.2631e-08 - val_accuracy: 0.9729 - val_loss: 0.1881\n",
      "Epoch 199/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 3.0788e-08 - val_accuracy: 0.9727 - val_loss: 0.1896\n",
      "Epoch 200/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.9289e-08 - val_accuracy: 0.9727 - val_loss: 0.1895\n",
      "Epoch 201/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.8417e-08 - val_accuracy: 0.9729 - val_loss: 0.1895\n",
      "Epoch 202/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.6268e-08 - val_accuracy: 0.9733 - val_loss: 0.1910\n",
      "Epoch 203/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.5229e-08 - val_accuracy: 0.9733 - val_loss: 0.1908\n",
      "Epoch 204/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.4639e-08 - val_accuracy: 0.9729 - val_loss: 0.1904\n",
      "Epoch 205/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.2964e-08 - val_accuracy: 0.9731 - val_loss: 0.1921\n",
      "Epoch 206/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.2419e-08 - val_accuracy: 0.9725 - val_loss: 0.1925\n",
      "Epoch 207/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.9873e-08 - val_accuracy: 0.9731 - val_loss: 0.1912\n",
      "Epoch 208/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.0253e-08 - val_accuracy: 0.9729 - val_loss: 0.1917\n",
      "Epoch 209/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.8803e-08 - val_accuracy: 0.9729 - val_loss: 0.1933\n",
      "Epoch 210/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.8464e-08 - val_accuracy: 0.9731 - val_loss: 0.1929\n",
      "Epoch 211/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.7393e-08 - val_accuracy: 0.9733 - val_loss: 0.1932\n",
      "Epoch 212/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.6594e-08 - val_accuracy: 0.9725 - val_loss: 0.1942\n",
      "Epoch 213/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.6238e-08 - val_accuracy: 0.9731 - val_loss: 0.1942\n",
      "Epoch 214/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.5341e-08 - val_accuracy: 0.9733 - val_loss: 0.1943\n",
      "Epoch 215/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.4839e-08 - val_accuracy: 0.9731 - val_loss: 0.1938\n",
      "Epoch 216/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.4413e-08 - val_accuracy: 0.9731 - val_loss: 0.1942\n",
      "Epoch 217/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.3426e-08 - val_accuracy: 0.9729 - val_loss: 0.1951\n",
      "Epoch 218/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.2912e-08 - val_accuracy: 0.9731 - val_loss: 0.1952\n",
      "Epoch 219/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.2581e-08 - val_accuracy: 0.9731 - val_loss: 0.1959\n",
      "Epoch 220/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.2319e-08 - val_accuracy: 0.9731 - val_loss: 0.1959\n",
      "Epoch 221/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.1248e-08 - val_accuracy: 0.9729 - val_loss: 0.1969\n",
      "Epoch 222/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.0773e-08 - val_accuracy: 0.9731 - val_loss: 0.1962\n",
      "Epoch 223/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.0647e-08 - val_accuracy: 0.9733 - val_loss: 0.1946\n",
      "Epoch 224/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.0725e-08 - val_accuracy: 0.9733 - val_loss: 0.1957\n",
      "Epoch 225/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.0084e-08 - val_accuracy: 0.9733 - val_loss: 0.1962\n",
      "Epoch 226/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 9.7189e-09 - val_accuracy: 0.9731 - val_loss: 0.1959\n",
      "Epoch 227/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 8.9598e-09 - val_accuracy: 0.9733 - val_loss: 0.1966\n",
      "Epoch 228/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 9.0085e-09 - val_accuracy: 0.9735 - val_loss: 0.1970\n",
      "Epoch 229/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 8.3487e-09 - val_accuracy: 0.9731 - val_loss: 0.1967\n",
      "Epoch 230/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 7.5004e-09 - val_accuracy: 0.9731 - val_loss: 0.1974\n",
      "Epoch 231/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 7.7315e-09 - val_accuracy: 0.9729 - val_loss: 0.1985\n",
      "Epoch 232/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 7.3299e-09 - val_accuracy: 0.9733 - val_loss: 0.1980\n",
      "Epoch 233/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 7.1850e-09 - val_accuracy: 0.9733 - val_loss: 0.1975\n",
      "Epoch 234/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 7.0486e-09 - val_accuracy: 0.9727 - val_loss: 0.1982\n",
      "Epoch 235/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 7.2025e-09 - val_accuracy: 0.9735 - val_loss: 0.1988\n",
      "Epoch 236/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 6.5378e-09 - val_accuracy: 0.9731 - val_loss: 0.1981\n",
      "Epoch 237/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 6.0770e-09 - val_accuracy: 0.9731 - val_loss: 0.1984\n",
      "Epoch 238/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.8641e-09 - val_accuracy: 0.9733 - val_loss: 0.1979\n",
      "Epoch 239/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 6.1785e-09 - val_accuracy: 0.9729 - val_loss: 0.1982\n",
      "Epoch 240/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 6.0652e-09 - val_accuracy: 0.9731 - val_loss: 0.1986\n",
      "Epoch 241/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.8591e-09 - val_accuracy: 0.9731 - val_loss: 0.1990\n",
      "Epoch 242/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.4301e-09 - val_accuracy: 0.9735 - val_loss: 0.1980\n",
      "Epoch 243/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.2863e-09 - val_accuracy: 0.9733 - val_loss: 0.1985\n",
      "Epoch 244/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.4035e-09 - val_accuracy: 0.9731 - val_loss: 0.1984\n",
      "Epoch 245/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.0069e-09 - val_accuracy: 0.9731 - val_loss: 0.1990\n",
      "Epoch 246/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.9392e-09 - val_accuracy: 0.9733 - val_loss: 0.1987\n",
      "Epoch 247/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.9012e-09 - val_accuracy: 0.9733 - val_loss: 0.1991\n",
      "Epoch 248/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.8340e-09 - val_accuracy: 0.9733 - val_loss: 0.1983\n",
      "Epoch 249/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.7271e-09 - val_accuracy: 0.9731 - val_loss: 0.1992\n",
      "Epoch 250/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.7334e-09 - val_accuracy: 0.9733 - val_loss: 0.1996\n",
      "Epoch 251/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.5193e-09 - val_accuracy: 0.9733 - val_loss: 0.1988\n",
      "Epoch 252/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.6211e-09 - val_accuracy: 0.9729 - val_loss: 0.1992\n",
      "Epoch 253/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.0111e-09 - val_accuracy: 0.9733 - val_loss: 0.1993\n",
      "Epoch 254/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.3386e-09 - val_accuracy: 0.9733 - val_loss: 0.1992\n",
      "Epoch 255/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.0259e-09 - val_accuracy: 0.9731 - val_loss: 0.1993\n",
      "Epoch 256/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.0617e-09 - val_accuracy: 0.9731 - val_loss: 0.1997\n",
      "Epoch 257/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.0945e-09 - val_accuracy: 0.9735 - val_loss: 0.1991\n",
      "Epoch 258/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.9635e-09 - val_accuracy: 0.9735 - val_loss: 0.1999\n",
      "Epoch 259/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.7663e-09 - val_accuracy: 0.9733 - val_loss: 0.1996\n",
      "Epoch 260/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.5791e-09 - val_accuracy: 0.9733 - val_loss: 0.1993\n",
      "Epoch 261/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.8192e-09 - val_accuracy: 0.9735 - val_loss: 0.1993\n",
      "Epoch 262/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.5220e-09 - val_accuracy: 0.9738 - val_loss: 0.1990\n",
      "Epoch 263/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.9228e-09 - val_accuracy: 0.9738 - val_loss: 0.1991\n",
      "Epoch 264/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.7079e-09 - val_accuracy: 0.9733 - val_loss: 0.1998\n",
      "Epoch 265/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.7443e-09 - val_accuracy: 0.9731 - val_loss: 0.2004\n",
      "Epoch 266/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.5556e-09 - val_accuracy: 0.9740 - val_loss: 0.1994\n",
      "Epoch 267/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.6683e-09 - val_accuracy: 0.9738 - val_loss: 0.1996\n",
      "Epoch 268/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.3805e-09 - val_accuracy: 0.9735 - val_loss: 0.1995\n",
      "Epoch 269/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.2438e-09 - val_accuracy: 0.9735 - val_loss: 0.1995\n",
      "Epoch 270/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.4544e-09 - val_accuracy: 0.9735 - val_loss: 0.1997\n",
      "Epoch 271/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.2822e-09 - val_accuracy: 0.9733 - val_loss: 0.1998\n",
      "Epoch 272/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.1907e-09 - val_accuracy: 0.9733 - val_loss: 0.1997\n",
      "Epoch 273/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.2783e-09 - val_accuracy: 0.9735 - val_loss: 0.1998\n",
      "Epoch 274/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.3313e-09 - val_accuracy: 0.9735 - val_loss: 0.1991\n",
      "Epoch 275/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.4036e-09 - val_accuracy: 0.9731 - val_loss: 0.1999\n",
      "Epoch 276/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.3918e-09 - val_accuracy: 0.9735 - val_loss: 0.1993\n",
      "Epoch 277/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.2370e-09 - val_accuracy: 0.9735 - val_loss: 0.1993\n",
      "Epoch 278/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.1352e-09 - val_accuracy: 0.9735 - val_loss: 0.1997\n",
      "Epoch 279/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.0356e-09 - val_accuracy: 0.9740 - val_loss: 0.1992\n",
      "Epoch 280/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.1737e-09 - val_accuracy: 0.9735 - val_loss: 0.1996\n",
      "Epoch 281/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.9173e-09 - val_accuracy: 0.9735 - val_loss: 0.1999\n",
      "Epoch 282/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.9701e-09 - val_accuracy: 0.9735 - val_loss: 0.2002\n",
      "Epoch 283/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.4521e-09 - val_accuracy: 0.9735 - val_loss: 0.1999\n",
      "Epoch 284/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.0275e-09 - val_accuracy: 0.9733 - val_loss: 0.1999\n",
      "Epoch 285/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.0224e-09 - val_accuracy: 0.9733 - val_loss: 0.2002\n",
      "Epoch 286/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.0496e-09 - val_accuracy: 0.9738 - val_loss: 0.1998\n",
      "Epoch 287/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.1549e-09 - val_accuracy: 0.9733 - val_loss: 0.1996\n",
      "Epoch 288/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.8886e-09 - val_accuracy: 0.9735 - val_loss: 0.1992\n",
      "Epoch 289/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.7469e-09 - val_accuracy: 0.9735 - val_loss: 0.1994\n",
      "Epoch 290/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.8901e-09 - val_accuracy: 0.9738 - val_loss: 0.1998\n",
      "Epoch 291/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.8495e-09 - val_accuracy: 0.9733 - val_loss: 0.1998\n",
      "Epoch 292/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.3091e-09 - val_accuracy: 0.9733 - val_loss: 0.1998\n",
      "Epoch 293/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.8524e-09 - val_accuracy: 0.9735 - val_loss: 0.1994\n",
      "Epoch 294/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.9378e-09 - val_accuracy: 0.9735 - val_loss: 0.1994\n",
      "Epoch 295/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.8232e-09 - val_accuracy: 0.9738 - val_loss: 0.1990\n",
      "Epoch 296/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.6731e-09 - val_accuracy: 0.9735 - val_loss: 0.1995\n",
      "Epoch 297/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.0139e-09 - val_accuracy: 0.9738 - val_loss: 0.1998\n",
      "Epoch 298/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.9889e-09 - val_accuracy: 0.9735 - val_loss: 0.1996\n",
      "Epoch 299/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.6749e-09 - val_accuracy: 0.9735 - val_loss: 0.1986\n",
      "Epoch 300/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.5313e-09 - val_accuracy: 0.9735 - val_loss: 0.1994\n",
      "Training time: 1716.335s\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9756 - loss: 0.1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97140, Test Time: 0.500s, Train Time: 1716.335s\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "hidden_units = seq_length // 8\n",
    "maxlen = 8\n",
    "num_blocks = 3\n",
    "num_epochs = 300\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "class NormalizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.layer_norm(inputs)\n",
    "\n",
    "class MultiheadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, num_heads=num_heads, dropout_rate=dropout_rate):\n",
    "        super(MultiheadAttentionLayer, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense_Q = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.dense_K = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.dense_V = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.normalize = NormalizeLayer()\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        return tf.concat(tf.split(x, self.num_heads, axis=-1), axis=0)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        return tf.concat(tf.split(x, self.num_heads, axis=0), axis=-1)\n",
    "\n",
    "    def call(self, queries, keys):\n",
    "        Q = self.dense_Q(queries)\n",
    "        K = self.dense_K(keys)\n",
    "        V = self.dense_V(keys)\n",
    "\n",
    "        Q_ = self.split_heads(Q)\n",
    "        K_ = self.split_heads(K)\n",
    "        V_ = self.split_heads(V)\n",
    "\n",
    "        scores = tf.matmul(Q_, K_, transpose_b=True) / tf.math.sqrt(tf.cast(K_.shape[-1], tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores)\n",
    "\n",
    "        query_masks = tf.cast(tf.reduce_sum(tf.abs(queries), axis=-1, keepdims=True) > 0, tf.float32)\n",
    "        query_masks = tf.tile(query_masks, [self.num_heads, 1, tf.shape(keys)[1]])\n",
    "        attention_weights *= query_masks\n",
    "\n",
    "        outputs = tf.matmul(attention_weights, V_)\n",
    "        outputs = self.merge_heads(outputs)\n",
    "        outputs += queries\n",
    "\n",
    "        return self.normalize(outputs)\n",
    "\n",
    "class FeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=num_units[0], kernel_size=1, activation=tf.nn.relu, use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=num_units[1], kernel_size=1, activation=None, use_bias=True)\n",
    "        self.normalize = NormalizeLayer()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs += inputs\n",
    "        return self.normalize(outputs)\n",
    "\n",
    "def one_hot_encoding(y_):\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    y_ = y_.reshape(-1, 1)\n",
    "    return encoder.fit_transform(y_)\n",
    "\n",
    "class MultiheadAttentionLayerWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, num_heads=num_heads, dropout_rate=dropout_rate):\n",
    "        super(MultiheadAttentionLayerWrapper, self).__init__()\n",
    "        self.multihead_attention = MultiheadAttentionLayer(num_units, num_heads, dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        queries, keys = inputs\n",
    "        return self.multihead_attention(queries, keys)\n",
    "\n",
    "# Define model\n",
    "def build_model():\n",
    "    inputs = tf.keras.Input(shape=(maxlen, hidden_units))\n",
    "    enc = inputs\n",
    "    for _ in range(num_blocks):\n",
    "        enc = MultiheadAttentionLayerWrapper(hidden_units)([enc, enc])\n",
    "    outputs = tf.keras.layers.Dense(6, activation='softmax')(tf.keras.layers.Flatten()(enc))\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = build_model()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Use existing data\n",
    "y_train = one_hot_encoding(y_train)\n",
    "y_val = one_hot_encoding(y_valuate)\n",
    "y_test = one_hot_encoding(y_test)\n",
    "\n",
    "# Define X_val\n",
    "X_val = X_valuate\n",
    "\n",
    "# Train the model\n",
    "time_start = time.time()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epochs)\n",
    "time_end = time.time()\n",
    "train_time = time_end - time_start\n",
    "print(f\"Training time: {train_time:.3f}s\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_time_start = time.time()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "test_time_end = time.time()\n",
    "test_time = test_time_end - test_time_start\n",
    "print(f\"Test Accuracy: {test_acc:.5f}, Test Time: {test_time:.3f}s, Train Time: {train_time:.3f}s\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"transformer_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.9.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.9.1\n",
      "c:\\Users\\Hi Windows 11 Home\\anaconda3\\envs\\SOTA\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.2512 - loss: 1.7005 - val_accuracy: 0.5512 - val_loss: 1.1083\n",
      "Epoch 2/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6539 - loss: 0.9193 - val_accuracy: 0.8067 - val_loss: 0.5190\n",
      "Epoch 3/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8175 - loss: 0.5131 - val_accuracy: 0.8423 - val_loss: 0.4277\n",
      "Epoch 4/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8832 - loss: 0.3347 - val_accuracy: 0.9187 - val_loss: 0.2500\n",
      "Epoch 5/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9103 - loss: 0.2688 - val_accuracy: 0.9257 - val_loss: 0.2163\n",
      "Epoch 6/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9231 - loss: 0.2216 - val_accuracy: 0.9334 - val_loss: 0.1921\n",
      "Epoch 7/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9392 - loss: 0.1820 - val_accuracy: 0.9300 - val_loss: 0.2024\n",
      "Epoch 8/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9381 - loss: 0.1832 - val_accuracy: 0.9370 - val_loss: 0.1842\n",
      "Epoch 9/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9507 - loss: 0.1393 - val_accuracy: 0.9535 - val_loss: 0.1418\n",
      "Epoch 10/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9534 - loss: 0.1349 - val_accuracy: 0.9396 - val_loss: 0.1856\n",
      "Epoch 11/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9587 - loss: 0.1245 - val_accuracy: 0.9413 - val_loss: 0.1764\n",
      "Epoch 12/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9626 - loss: 0.1118 - val_accuracy: 0.9586 - val_loss: 0.1259\n",
      "Epoch 13/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9633 - loss: 0.1009 - val_accuracy: 0.9569 - val_loss: 0.1283\n",
      "Epoch 14/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9676 - loss: 0.0923 - val_accuracy: 0.9567 - val_loss: 0.1284\n",
      "Epoch 15/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9756 - loss: 0.0751 - val_accuracy: 0.9618 - val_loss: 0.1186\n",
      "Epoch 16/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9734 - loss: 0.0850 - val_accuracy: 0.9565 - val_loss: 0.1302\n",
      "Epoch 17/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9790 - loss: 0.0616 - val_accuracy: 0.9526 - val_loss: 0.1548\n",
      "Epoch 18/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9808 - loss: 0.0554 - val_accuracy: 0.9673 - val_loss: 0.1045\n",
      "Epoch 19/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9837 - loss: 0.0473 - val_accuracy: 0.9725 - val_loss: 0.0875\n",
      "Epoch 20/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9888 - loss: 0.0336 - val_accuracy: 0.9716 - val_loss: 0.0921\n",
      "Epoch 21/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9776 - loss: 0.0646 - val_accuracy: 0.9735 - val_loss: 0.0806\n",
      "Epoch 22/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9930 - loss: 0.0232 - val_accuracy: 0.9742 - val_loss: 0.0888\n",
      "Epoch 23/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9895 - loss: 0.0311 - val_accuracy: 0.9693 - val_loss: 0.0992\n",
      "Epoch 24/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9851 - loss: 0.0442 - val_accuracy: 0.9723 - val_loss: 0.0862\n",
      "Epoch 25/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9907 - loss: 0.0276 - val_accuracy: 0.9661 - val_loss: 0.1139\n",
      "Epoch 26/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9890 - loss: 0.0293 - val_accuracy: 0.9684 - val_loss: 0.0987\n",
      "Epoch 27/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9836 - loss: 0.0488 - val_accuracy: 0.9784 - val_loss: 0.0709\n",
      "Epoch 28/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9935 - loss: 0.0182 - val_accuracy: 0.9774 - val_loss: 0.0775\n",
      "Epoch 29/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9939 - loss: 0.0185 - val_accuracy: 0.9765 - val_loss: 0.0827\n",
      "Epoch 30/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9944 - loss: 0.0176 - val_accuracy: 0.9816 - val_loss: 0.0667\n",
      "Epoch 31/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9941 - loss: 0.0176 - val_accuracy: 0.9748 - val_loss: 0.0891\n",
      "Epoch 32/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9880 - loss: 0.0373 - val_accuracy: 0.9806 - val_loss: 0.0702\n",
      "Epoch 33/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9945 - loss: 0.0162 - val_accuracy: 0.9765 - val_loss: 0.0807\n",
      "Epoch 34/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9957 - loss: 0.0137 - val_accuracy: 0.9720 - val_loss: 0.0916\n",
      "Epoch 35/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9969 - loss: 0.0102 - val_accuracy: 0.9784 - val_loss: 0.0744\n",
      "Epoch 36/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9957 - loss: 0.0114 - val_accuracy: 0.9584 - val_loss: 0.1314\n",
      "Epoch 37/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9795 - loss: 0.0589 - val_accuracy: 0.9791 - val_loss: 0.0701\n",
      "Epoch 38/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9904 - loss: 0.0296 - val_accuracy: 0.9802 - val_loss: 0.0660\n",
      "Epoch 39/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9976 - loss: 0.0102 - val_accuracy: 0.9767 - val_loss: 0.0773\n",
      "Epoch 40/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9955 - loss: 0.0128 - val_accuracy: 0.9776 - val_loss: 0.0729\n",
      "Epoch 41/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9959 - loss: 0.0123 - val_accuracy: 0.9765 - val_loss: 0.0813\n",
      "Epoch 42/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9939 - loss: 0.0169 - val_accuracy: 0.9795 - val_loss: 0.0694\n",
      "Epoch 43/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9970 - loss: 0.0102 - val_accuracy: 0.9752 - val_loss: 0.0925\n",
      "Epoch 44/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9977 - loss: 0.0075 - val_accuracy: 0.9806 - val_loss: 0.0695\n",
      "Epoch 45/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9944 - loss: 0.0137 - val_accuracy: 0.9797 - val_loss: 0.0723\n",
      "Epoch 46/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9958 - loss: 0.0132 - val_accuracy: 0.9787 - val_loss: 0.0791\n",
      "Epoch 47/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9976 - loss: 0.0080 - val_accuracy: 0.9742 - val_loss: 0.0971\n",
      "Epoch 48/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9953 - loss: 0.0134 - val_accuracy: 0.9748 - val_loss: 0.0925\n",
      "Epoch 49/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 0.9787 - val_loss: 0.0811\n",
      "Epoch 50/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9974 - loss: 0.0093 - val_accuracy: 0.9814 - val_loss: 0.0618\n",
      "Epoch 51/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9971 - loss: 0.0088 - val_accuracy: 0.9727 - val_loss: 0.0914\n",
      "Epoch 52/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9947 - loss: 0.0154 - val_accuracy: 0.9750 - val_loss: 0.0880\n",
      "Epoch 53/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0092 - val_accuracy: 0.9846 - val_loss: 0.0575\n",
      "Epoch 54/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 9.0704e-04 - val_accuracy: 0.9842 - val_loss: 0.0620\n",
      "Epoch 55/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 7.4953e-04 - val_accuracy: 0.9836 - val_loss: 0.0644\n",
      "Epoch 56/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9962 - loss: 0.0128 - val_accuracy: 0.9797 - val_loss: 0.0761\n",
      "Epoch 57/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 0.9829 - val_loss: 0.0657\n",
      "Epoch 58/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9980 - loss: 0.0057 - val_accuracy: 0.9774 - val_loss: 0.0941\n",
      "Epoch 59/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 0.9846 - val_loss: 0.0623\n",
      "Epoch 60/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9989 - loss: 0.0048 - val_accuracy: 0.9609 - val_loss: 0.1303\n",
      "Epoch 61/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9934 - loss: 0.0181 - val_accuracy: 0.9748 - val_loss: 0.0972\n",
      "Epoch 62/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9951 - loss: 0.0136 - val_accuracy: 0.9819 - val_loss: 0.0759\n",
      "Epoch 63/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9810 - val_loss: 0.0726\n",
      "Epoch 64/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9980 - loss: 0.0054 - val_accuracy: 0.9731 - val_loss: 0.0994\n",
      "Epoch 65/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9954 - loss: 0.0151 - val_accuracy: 0.9802 - val_loss: 0.0719\n",
      "Epoch 66/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 0.9819 - val_loss: 0.0771\n",
      "Epoch 67/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9838 - val_loss: 0.0618\n",
      "Epoch 68/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 8.6577e-04 - val_accuracy: 0.9795 - val_loss: 0.0860\n",
      "Epoch 69/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9981 - loss: 0.0058 - val_accuracy: 0.9810 - val_loss: 0.0711\n",
      "Epoch 70/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 0.9802 - val_loss: 0.0736\n",
      "Epoch 71/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9970 - loss: 0.0109 - val_accuracy: 0.9797 - val_loss: 0.0769\n",
      "Epoch 72/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9983 - loss: 0.0063 - val_accuracy: 0.9829 - val_loss: 0.0584\n",
      "Epoch 73/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0031 - val_accuracy: 0.9855 - val_loss: 0.0544\n",
      "Epoch 74/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.5534e-04 - val_accuracy: 0.9851 - val_loss: 0.0533\n",
      "Epoch 75/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 0.9795 - val_loss: 0.0729\n",
      "Epoch 76/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9912 - loss: 0.0252 - val_accuracy: 0.9806 - val_loss: 0.0637\n",
      "Epoch 77/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9969 - loss: 0.0097 - val_accuracy: 0.9859 - val_loss: 0.0468\n",
      "Epoch 78/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 0.9799 - val_loss: 0.0706\n",
      "Epoch 79/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 0.9821 - val_loss: 0.0656\n",
      "Epoch 80/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9872 - val_loss: 0.0532\n",
      "Epoch 81/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.9821 - val_loss: 0.0752\n",
      "Epoch 82/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9982 - loss: 0.0057 - val_accuracy: 0.9787 - val_loss: 0.0745\n",
      "Epoch 83/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 0.9727 - val_loss: 0.1015\n",
      "Epoch 84/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9987 - loss: 0.0038 - val_accuracy: 0.9814 - val_loss: 0.0735\n",
      "Epoch 85/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9787 - val_loss: 0.0850\n",
      "Epoch 86/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9971 - loss: 0.0107 - val_accuracy: 0.9806 - val_loss: 0.0786\n",
      "Epoch 87/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 0.9761 - val_loss: 0.0831\n",
      "Epoch 88/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9976 - loss: 0.0065 - val_accuracy: 0.9868 - val_loss: 0.0494\n",
      "Epoch 89/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9986 - loss: 0.0039 - val_accuracy: 0.9774 - val_loss: 0.0953\n",
      "Epoch 90/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.9876 - val_loss: 0.0471\n",
      "Epoch 91/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 0.9863 - val_loss: 0.0549\n",
      "Epoch 92/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.9866 - val_loss: 0.0497\n",
      "Epoch 93/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9880 - val_loss: 0.0425\n",
      "Epoch 94/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3228e-04 - val_accuracy: 0.9883 - val_loss: 0.0430\n",
      "Epoch 95/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 8.4298e-05 - val_accuracy: 0.9883 - val_loss: 0.0435\n",
      "Epoch 96/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 8.8833e-05 - val_accuracy: 0.9889 - val_loss: 0.0428\n",
      "Epoch 97/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.4330e-05 - val_accuracy: 0.9900 - val_loss: 0.0426\n",
      "Epoch 98/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 7.9046e-05 - val_accuracy: 0.9898 - val_loss: 0.0445\n",
      "Epoch 99/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.5067e-05 - val_accuracy: 0.9887 - val_loss: 0.0463\n",
      "Epoch 100/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.6088e-05 - val_accuracy: 0.9889 - val_loss: 0.0470\n",
      "Epoch 101/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.5502e-05 - val_accuracy: 0.9883 - val_loss: 0.0486\n",
      "Epoch 102/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.5749e-05 - val_accuracy: 0.9887 - val_loss: 0.0457\n",
      "Epoch 103/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 3.3911e-05 - val_accuracy: 0.9887 - val_loss: 0.0463\n",
      "Epoch 104/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.7932e-05 - val_accuracy: 0.9880 - val_loss: 0.0461\n",
      "Epoch 105/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.1283e-05 - val_accuracy: 0.9876 - val_loss: 0.0496\n",
      "Epoch 106/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.7701e-05 - val_accuracy: 0.9878 - val_loss: 0.0485\n",
      "Epoch 107/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.4619e-05 - val_accuracy: 0.9889 - val_loss: 0.0481\n",
      "Epoch 108/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.1783e-05 - val_accuracy: 0.9880 - val_loss: 0.0541\n",
      "Epoch 109/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.2339e-05 - val_accuracy: 0.9885 - val_loss: 0.0515\n",
      "Epoch 110/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.9234e-05 - val_accuracy: 0.9883 - val_loss: 0.0511\n",
      "Epoch 111/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.3230e-05 - val_accuracy: 0.9885 - val_loss: 0.0525\n",
      "Epoch 112/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.2505e-05 - val_accuracy: 0.9885 - val_loss: 0.0515\n",
      "Epoch 113/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 0.9377 - val_loss: 0.2254\n",
      "Epoch 114/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9788 - loss: 0.0640 - val_accuracy: 0.9821 - val_loss: 0.0644\n",
      "Epoch 115/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9977 - loss: 0.0070 - val_accuracy: 0.9842 - val_loss: 0.0544\n",
      "Epoch 116/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 0.9827 - val_loss: 0.0623\n",
      "Epoch 117/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.9797 - val_loss: 0.0650\n",
      "Epoch 118/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 0.9844 - val_loss: 0.0588\n",
      "Epoch 119/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9819 - val_loss: 0.0653\n",
      "Epoch 120/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9729 - val_loss: 0.0911\n",
      "Epoch 121/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9978 - loss: 0.0085 - val_accuracy: 0.9834 - val_loss: 0.0553\n",
      "Epoch 122/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 0.9846 - val_loss: 0.0566\n",
      "Epoch 123/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9868 - val_loss: 0.0469\n",
      "Epoch 124/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.3948e-04 - val_accuracy: 0.9876 - val_loss: 0.0508\n",
      "Epoch 125/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 0.9876 - val_loss: 0.0431\n",
      "Epoch 126/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 0.9793 - val_loss: 0.0790\n",
      "Epoch 127/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9964 - loss: 0.0098 - val_accuracy: 0.9861 - val_loss: 0.0602\n",
      "Epoch 128/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9857 - val_loss: 0.0537\n",
      "Epoch 129/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 6.2233e-04 - val_accuracy: 0.9859 - val_loss: 0.0496\n",
      "Epoch 130/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0034 - val_accuracy: 0.9844 - val_loss: 0.0608\n",
      "Epoch 131/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9926 - loss: 0.0251 - val_accuracy: 0.9816 - val_loss: 0.0684\n",
      "Epoch 132/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9986 - loss: 0.0029 - val_accuracy: 0.9868 - val_loss: 0.0504\n",
      "Epoch 133/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9863 - val_loss: 0.0510\n",
      "Epoch 134/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9876 - val_loss: 0.0462\n",
      "Epoch 135/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.0922e-04 - val_accuracy: 0.9866 - val_loss: 0.0547\n",
      "Epoch 136/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.8822e-04 - val_accuracy: 0.9883 - val_loss: 0.0459\n",
      "Epoch 137/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 4.4214e-04 - val_accuracy: 0.9814 - val_loss: 0.0721\n",
      "Epoch 138/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0022 - val_accuracy: 0.9829 - val_loss: 0.0785\n",
      "Epoch 139/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 0.9767 - val_loss: 0.0942\n",
      "Epoch 140/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 0.9834 - val_loss: 0.0615\n",
      "Epoch 141/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 0.9883 - val_loss: 0.0472\n",
      "Epoch 142/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 5.1995e-04 - val_accuracy: 0.9880 - val_loss: 0.0424\n",
      "Epoch 143/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 6.8772e-04 - val_accuracy: 0.9861 - val_loss: 0.0631\n",
      "Epoch 144/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 0.9772 - val_loss: 0.0820\n",
      "Epoch 145/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9885 - val_loss: 0.0367\n",
      "Epoch 146/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 7.0142e-04 - val_accuracy: 0.9906 - val_loss: 0.0371\n",
      "Epoch 147/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5711e-04 - val_accuracy: 0.9900 - val_loss: 0.0386\n",
      "Epoch 148/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.3357e-04 - val_accuracy: 0.9908 - val_loss: 0.0361\n",
      "Epoch 149/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.4982e-05 - val_accuracy: 0.9908 - val_loss: 0.0369\n",
      "Epoch 150/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.6538e-05 - val_accuracy: 0.9913 - val_loss: 0.0372\n",
      "Epoch 151/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.6557e-05 - val_accuracy: 0.9908 - val_loss: 0.0377\n",
      "Epoch 152/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.5500e-05 - val_accuracy: 0.9908 - val_loss: 0.0386\n",
      "Epoch 153/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.6760e-05 - val_accuracy: 0.9904 - val_loss: 0.0383\n",
      "Epoch 154/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.0635e-05 - val_accuracy: 0.9906 - val_loss: 0.0385\n",
      "Epoch 155/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.4867e-05 - val_accuracy: 0.9906 - val_loss: 0.0386\n",
      "Epoch 156/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.4822e-05 - val_accuracy: 0.9906 - val_loss: 0.0389\n",
      "Epoch 157/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.6263e-05 - val_accuracy: 0.9904 - val_loss: 0.0390\n",
      "Epoch 158/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.2394e-05 - val_accuracy: 0.9902 - val_loss: 0.0396\n",
      "Epoch 159/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.1362e-05 - val_accuracy: 0.9900 - val_loss: 0.0394\n",
      "Epoch 160/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.0712e-05 - val_accuracy: 0.9910 - val_loss: 0.0392\n",
      "Epoch 161/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.2811e-05 - val_accuracy: 0.9889 - val_loss: 0.0440\n",
      "Epoch 162/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.3533e-05 - val_accuracy: 0.9898 - val_loss: 0.0407\n",
      "Epoch 163/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.1929e-05 - val_accuracy: 0.9900 - val_loss: 0.0411\n",
      "Epoch 164/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.3166e-05 - val_accuracy: 0.9904 - val_loss: 0.0450\n",
      "Epoch 165/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.6563e-05 - val_accuracy: 0.9902 - val_loss: 0.0434\n",
      "Epoch 166/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5506e-05 - val_accuracy: 0.9898 - val_loss: 0.0452\n",
      "Epoch 167/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9980 - loss: 0.0076 - val_accuracy: 0.9821 - val_loss: 0.0613\n",
      "Epoch 168/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9976 - loss: 0.0081 - val_accuracy: 0.9851 - val_loss: 0.0519\n",
      "Epoch 169/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 0.9825 - val_loss: 0.0649\n",
      "Epoch 170/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 0.9793 - val_loss: 0.0753\n",
      "Epoch 171/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0019 - val_accuracy: 0.9851 - val_loss: 0.0516\n",
      "Epoch 172/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 0.9885 - val_loss: 0.0432\n",
      "Epoch 173/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 0.9883 - val_loss: 0.0432\n",
      "Epoch 174/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 4.4335e-04 - val_accuracy: 0.9895 - val_loss: 0.0377\n",
      "Epoch 175/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5478e-04 - val_accuracy: 0.9898 - val_loss: 0.0403\n",
      "Epoch 176/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.1079e-04 - val_accuracy: 0.9893 - val_loss: 0.0394\n",
      "Epoch 177/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3129e-04 - val_accuracy: 0.9906 - val_loss: 0.0357\n",
      "Epoch 178/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 6.1933e-04 - val_accuracy: 0.9827 - val_loss: 0.0675\n",
      "Epoch 179/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 0.9823 - val_loss: 0.0702\n",
      "Epoch 180/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 0.9842 - val_loss: 0.0739\n",
      "Epoch 181/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9836 - val_loss: 0.0854\n",
      "Epoch 182/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9983 - loss: 0.0074 - val_accuracy: 0.9846 - val_loss: 0.0640\n",
      "Epoch 183/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 0.9831 - val_loss: 0.0731\n",
      "Epoch 184/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9985 - loss: 0.0044 - val_accuracy: 0.9863 - val_loss: 0.0526\n",
      "Epoch 185/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 0.9857 - val_loss: 0.0506\n",
      "Epoch 186/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0068 - val_accuracy: 0.9804 - val_loss: 0.0787\n",
      "Epoch 187/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 0.9883 - val_loss: 0.0456\n",
      "Epoch 188/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.1016e-04 - val_accuracy: 0.9906 - val_loss: 0.0369\n",
      "Epoch 189/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5524e-04 - val_accuracy: 0.9915 - val_loss: 0.0346\n",
      "Epoch 190/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 3.7368e-04 - val_accuracy: 0.9895 - val_loss: 0.0414\n",
      "Epoch 191/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.7856e-05 - val_accuracy: 0.9904 - val_loss: 0.0388\n",
      "Epoch 192/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.1021e-04 - val_accuracy: 0.9902 - val_loss: 0.0398\n",
      "Epoch 193/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.2862e-05 - val_accuracy: 0.9913 - val_loss: 0.0378\n",
      "Epoch 194/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 3.4118e-05 - val_accuracy: 0.9908 - val_loss: 0.0387\n",
      "Epoch 195/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.6848e-05 - val_accuracy: 0.9910 - val_loss: 0.0394\n",
      "Epoch 196/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.7998e-05 - val_accuracy: 0.9870 - val_loss: 0.0513\n",
      "Epoch 197/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9906 - val_loss: 0.0414\n",
      "Epoch 198/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3704e-04 - val_accuracy: 0.9902 - val_loss: 0.0381\n",
      "Epoch 199/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 5.1312e-04 - val_accuracy: 0.9787 - val_loss: 0.0984\n",
      "Epoch 200/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9943 - loss: 0.0181 - val_accuracy: 0.9851 - val_loss: 0.0539\n",
      "Epoch 201/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9988 - loss: 0.0032 - val_accuracy: 0.9880 - val_loss: 0.0439\n",
      "Epoch 202/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.9902 - val_loss: 0.0387\n",
      "Epoch 203/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 4.8388e-04 - val_accuracy: 0.9906 - val_loss: 0.0322\n",
      "Epoch 204/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 7.4441e-04 - val_accuracy: 0.9874 - val_loss: 0.0522\n",
      "Epoch 205/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 0.9883 - val_loss: 0.0414\n",
      "Epoch 206/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 8.0156e-04 - val_accuracy: 0.9891 - val_loss: 0.0381\n",
      "Epoch 207/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 3.9932e-04 - val_accuracy: 0.9712 - val_loss: 0.1416\n",
      "Epoch 208/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0089 - val_accuracy: 0.9868 - val_loss: 0.0576\n",
      "Epoch 209/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9976 - loss: 0.0056 - val_accuracy: 0.9831 - val_loss: 0.0701\n",
      "Epoch 210/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 0.9887 - val_loss: 0.0420\n",
      "Epoch 211/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.8287e-04 - val_accuracy: 0.9902 - val_loss: 0.0412\n",
      "Epoch 212/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.4485e-04 - val_accuracy: 0.9904 - val_loss: 0.0400\n",
      "Epoch 213/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.6527e-04 - val_accuracy: 0.9889 - val_loss: 0.0513\n",
      "Epoch 214/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9825 - val_loss: 0.0758\n",
      "Epoch 215/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 0.9893 - val_loss: 0.0382\n",
      "Epoch 216/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 6.0173e-04 - val_accuracy: 0.9866 - val_loss: 0.0606\n",
      "Epoch 217/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9876 - val_loss: 0.0500\n",
      "Epoch 218/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.4117e-04 - val_accuracy: 0.9893 - val_loss: 0.0451\n",
      "Epoch 219/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3439e-04 - val_accuracy: 0.9872 - val_loss: 0.0573\n",
      "Epoch 220/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 0.9836 - val_loss: 0.0689\n",
      "Epoch 221/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9988 - loss: 0.0031 - val_accuracy: 0.9880 - val_loss: 0.0554\n",
      "Epoch 222/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.1171e-04 - val_accuracy: 0.9900 - val_loss: 0.0466\n",
      "Epoch 223/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.7979e-05 - val_accuracy: 0.9895 - val_loss: 0.0497\n",
      "Epoch 224/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.7335e-05 - val_accuracy: 0.9887 - val_loss: 0.0519\n",
      "Epoch 225/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0015 - val_accuracy: 0.9878 - val_loss: 0.0518\n",
      "Epoch 226/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.4805e-04 - val_accuracy: 0.9889 - val_loss: 0.0508\n",
      "Epoch 227/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0010 - val_accuracy: 0.9866 - val_loss: 0.0627\n",
      "Epoch 228/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9959 - loss: 0.0166 - val_accuracy: 0.9859 - val_loss: 0.0566\n",
      "Epoch 229/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.9885 - val_loss: 0.0419\n",
      "Epoch 230/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.6374e-04 - val_accuracy: 0.9900 - val_loss: 0.0395\n",
      "Epoch 231/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3858e-04 - val_accuracy: 0.9893 - val_loss: 0.0413\n",
      "Epoch 232/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.4596e-05 - val_accuracy: 0.9889 - val_loss: 0.0432\n",
      "Epoch 233/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.1570e-04 - val_accuracy: 0.9891 - val_loss: 0.0457\n",
      "Epoch 234/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 9.9201e-05 - val_accuracy: 0.9900 - val_loss: 0.0432\n",
      "Epoch 235/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.2767e-05 - val_accuracy: 0.9900 - val_loss: 0.0434\n",
      "Epoch 236/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.7547e-05 - val_accuracy: 0.9900 - val_loss: 0.0444\n",
      "Epoch 237/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.5316e-05 - val_accuracy: 0.9895 - val_loss: 0.0457\n",
      "Epoch 238/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.3308e-05 - val_accuracy: 0.9902 - val_loss: 0.0470\n",
      "Epoch 239/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.0399e-05 - val_accuracy: 0.9898 - val_loss: 0.0471\n",
      "Epoch 240/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.0669e-05 - val_accuracy: 0.9895 - val_loss: 0.0497\n",
      "Epoch 241/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.2058e-04 - val_accuracy: 0.9891 - val_loss: 0.0559\n",
      "Epoch 242/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9952 - loss: 0.0152 - val_accuracy: 0.9855 - val_loss: 0.0546\n",
      "Epoch 243/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 0.9842 - val_loss: 0.0567\n",
      "Epoch 244/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9977 - loss: 0.0072 - val_accuracy: 0.9889 - val_loss: 0.0492\n",
      "Epoch 245/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 6.8448e-04 - val_accuracy: 0.9898 - val_loss: 0.0454\n",
      "Epoch 246/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.4380e-04 - val_accuracy: 0.9876 - val_loss: 0.0546\n",
      "Epoch 247/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.7481e-04 - val_accuracy: 0.9874 - val_loss: 0.0691\n",
      "Epoch 248/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9983 - loss: 0.0071 - val_accuracy: 0.9895 - val_loss: 0.0450\n",
      "Epoch 249/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 7.6882e-04 - val_accuracy: 0.9863 - val_loss: 0.0553\n",
      "Epoch 250/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 0.9889 - val_loss: 0.0433\n",
      "Epoch 251/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.7276e-04 - val_accuracy: 0.9891 - val_loss: 0.0416\n",
      "Epoch 252/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.9786e-05 - val_accuracy: 0.9902 - val_loss: 0.0415\n",
      "Epoch 253/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.7540e-05 - val_accuracy: 0.9900 - val_loss: 0.0418\n",
      "Epoch 254/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.1295e-05 - val_accuracy: 0.9895 - val_loss: 0.0481\n",
      "Epoch 255/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 5.2378e-05 - val_accuracy: 0.9902 - val_loss: 0.0398\n",
      "Epoch 256/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.9486e-05 - val_accuracy: 0.9906 - val_loss: 0.0418\n",
      "Epoch 257/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.4551e-05 - val_accuracy: 0.9904 - val_loss: 0.0428\n",
      "Epoch 258/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.8080e-05 - val_accuracy: 0.9902 - val_loss: 0.0425\n",
      "Epoch 259/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.6002e-05 - val_accuracy: 0.9906 - val_loss: 0.0417\n",
      "Epoch 260/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.1210e-05 - val_accuracy: 0.9904 - val_loss: 0.0451\n",
      "Epoch 261/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 3.6544e-05 - val_accuracy: 0.9906 - val_loss: 0.0434\n",
      "Epoch 262/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5539e-05 - val_accuracy: 0.9906 - val_loss: 0.0426\n",
      "Epoch 263/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5368e-05 - val_accuracy: 0.9900 - val_loss: 0.0459\n",
      "Epoch 264/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.6622e-05 - val_accuracy: 0.9904 - val_loss: 0.0471\n",
      "Epoch 265/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.0241e-05 - val_accuracy: 0.9893 - val_loss: 0.0469\n",
      "Epoch 266/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.6821e-05 - val_accuracy: 0.9902 - val_loss: 0.0463\n",
      "Epoch 267/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.2667e-05 - val_accuracy: 0.9904 - val_loss: 0.0451\n",
      "Epoch 268/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.9869e-06 - val_accuracy: 0.9904 - val_loss: 0.0447\n",
      "Epoch 269/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.2825e-05 - val_accuracy: 0.9908 - val_loss: 0.0461\n",
      "Epoch 270/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.0605e-05 - val_accuracy: 0.9910 - val_loss: 0.0426\n",
      "Epoch 271/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9582 - val_loss: 0.2224\n",
      "Epoch 272/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9884 - loss: 0.0417 - val_accuracy: 0.9883 - val_loss: 0.0403\n",
      "Epoch 273/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 9.9864e-04 - val_accuracy: 0.9908 - val_loss: 0.0364\n",
      "Epoch 274/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.3925e-04 - val_accuracy: 0.9895 - val_loss: 0.0362\n",
      "Epoch 275/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.2509e-04 - val_accuracy: 0.9900 - val_loss: 0.0357\n",
      "Epoch 276/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.0114e-05 - val_accuracy: 0.9902 - val_loss: 0.0345\n",
      "Epoch 277/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 8.0449e-05 - val_accuracy: 0.9904 - val_loss: 0.0359\n",
      "Epoch 278/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.6238e-05 - val_accuracy: 0.9906 - val_loss: 0.0360\n",
      "Epoch 279/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 5.5573e-05 - val_accuracy: 0.9902 - val_loss: 0.0371\n",
      "Epoch 280/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.4378e-05 - val_accuracy: 0.9887 - val_loss: 0.0449\n",
      "Epoch 281/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9999 - loss: 3.8081e-04 - val_accuracy: 0.9891 - val_loss: 0.0498\n",
      "Epoch 282/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 7.3703e-04 - val_accuracy: 0.9874 - val_loss: 0.0524\n",
      "Epoch 283/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0039 - val_accuracy: 0.9840 - val_loss: 0.0616\n",
      "Epoch 284/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9961 - loss: 0.0155 - val_accuracy: 0.9846 - val_loss: 0.0597\n",
      "Epoch 285/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.9853 - val_loss: 0.0574\n",
      "Epoch 286/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.8231e-04 - val_accuracy: 0.9880 - val_loss: 0.0446\n",
      "Epoch 287/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3372e-04 - val_accuracy: 0.9893 - val_loss: 0.0435\n",
      "Epoch 288/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 7.1112e-05 - val_accuracy: 0.9889 - val_loss: 0.0435\n",
      "Epoch 289/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 4.8533e-05 - val_accuracy: 0.9891 - val_loss: 0.0426\n",
      "Epoch 290/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.3162e-04 - val_accuracy: 0.9880 - val_loss: 0.0470\n",
      "Epoch 291/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0014 - val_accuracy: 0.9874 - val_loss: 0.0475\n",
      "Epoch 292/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9902 - val_loss: 0.0400\n",
      "Epoch 293/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.7930e-04 - val_accuracy: 0.9898 - val_loss: 0.0406\n",
      "Epoch 294/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 9.9327e-04 - val_accuracy: 0.9846 - val_loss: 0.0620\n",
      "Epoch 295/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9973 - loss: 0.0090 - val_accuracy: 0.9848 - val_loss: 0.0658\n",
      "Epoch 296/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9982 - loss: 0.0084 - val_accuracy: 0.9887 - val_loss: 0.0392\n",
      "Epoch 297/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.0144e-04 - val_accuracy: 0.9900 - val_loss: 0.0392\n",
      "Epoch 298/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 8.3587e-04 - val_accuracy: 0.9913 - val_loss: 0.0325\n",
      "Epoch 299/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 6.0837e-05 - val_accuracy: 0.9919 - val_loss: 0.0313\n",
      "Epoch 300/300\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 6.5336e-05 - val_accuracy: 0.9923 - val_loss: 0.0298\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9896 - loss: 0.0521\n",
      "Test Accuracy: 0.9885\n",
      "time taken to train LSTM model: 494.97142934799194\n",
      "time taken to test LSTM model: 0.15733814239501953\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.9.1\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define LSTM model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]), y_train.shape[1])\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_LSTM_train_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=300, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "end_LSTM_train_time = time.time()\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "start_LSTM_test_time = time.time()\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "end_LSTM_test_time = time.time()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f'time taken to train LSTM model: {end_LSTM_train_time - start_LSTM_train_time}')\n",
    "print(f'time taken to test LSTM model: {end_LSTM_test_time - start_LSTM_test_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "23436 23998464 23997440\n",
      "Total data volume: 23436\n",
      "Train data volume: 21090, Val data volume: 1172, Test data volume: 1172\n",
      "Training...\n",
      "\n",
      "Epoch 1/300\n",
      "165/165 - 36s - 217ms/step - loss: 0.1325 - val_loss: 0.0519\n",
      "Epoch 2/300\n",
      "165/165 - 23s - 140ms/step - loss: 0.0545 - val_loss: 0.0330\n",
      "Epoch 3/300\n",
      "165/165 - 24s - 146ms/step - loss: 0.0374 - val_loss: 0.0239\n",
      "Epoch 4/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0299 - val_loss: 0.0208\n",
      "Epoch 5/300\n",
      "165/165 - 24s - 147ms/step - loss: 0.0266 - val_loss: 0.0193\n",
      "Epoch 6/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0245 - val_loss: 0.0183\n",
      "Epoch 7/300\n",
      "165/165 - 24s - 146ms/step - loss: 0.0230 - val_loss: 0.0174\n",
      "Epoch 8/300\n",
      "165/165 - 24s - 148ms/step - loss: 0.0217 - val_loss: 0.0166\n",
      "Epoch 9/300\n",
      "165/165 - 24s - 146ms/step - loss: 0.0206 - val_loss: 0.0157\n",
      "Epoch 10/300\n",
      "165/165 - 24s - 148ms/step - loss: 0.0196 - val_loss: 0.0149\n",
      "Epoch 11/300\n",
      "165/165 - 24s - 147ms/step - loss: 0.0186 - val_loss: 0.0141\n",
      "Epoch 12/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0178 - val_loss: 0.0134\n",
      "Epoch 13/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0169 - val_loss: 0.0126\n",
      "Epoch 14/300\n",
      "165/165 - 24s - 148ms/step - loss: 0.0162 - val_loss: 0.0119\n",
      "Epoch 15/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0154 - val_loss: 0.0113\n",
      "Epoch 16/300\n",
      "165/165 - 24s - 148ms/step - loss: 0.0147 - val_loss: 0.0106\n",
      "Epoch 17/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0140 - val_loss: 0.0100\n",
      "Epoch 18/300\n",
      "165/165 - 25s - 149ms/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 19/300\n",
      "165/165 - 24s - 148ms/step - loss: 0.0128 - val_loss: 0.0090\n",
      "Epoch 20/300\n",
      "165/165 - 24s - 146ms/step - loss: 0.0123 - val_loss: 0.0085\n",
      "Epoch 21/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0118 - val_loss: 0.0080\n",
      "Epoch 22/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0113 - val_loss: 0.0077\n",
      "Epoch 23/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0109 - val_loss: 0.0073\n",
      "Epoch 24/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0105 - val_loss: 0.0070\n",
      "Epoch 25/300\n",
      "165/165 - 25s - 149ms/step - loss: 0.0101 - val_loss: 0.0066\n",
      "Epoch 26/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0098 - val_loss: 0.0063\n",
      "Epoch 27/300\n",
      "165/165 - 25s - 149ms/step - loss: 0.0094 - val_loss: 0.0061\n",
      "Epoch 28/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0091 - val_loss: 0.0058\n",
      "Epoch 29/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0089 - val_loss: 0.0057\n",
      "Epoch 30/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0086 - val_loss: 0.0053\n",
      "Epoch 31/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0083 - val_loss: 0.0051\n",
      "Epoch 32/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0081 - val_loss: 0.0050\n",
      "Epoch 33/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0078 - val_loss: 0.0048\n",
      "Epoch 34/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0076 - val_loss: 0.0046\n",
      "Epoch 35/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0074 - val_loss: 0.0045\n",
      "Epoch 36/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0072 - val_loss: 0.0042\n",
      "Epoch 37/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0070 - val_loss: 0.0041\n",
      "Epoch 38/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0068 - val_loss: 0.0040\n",
      "Epoch 39/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0067 - val_loss: 0.0039\n",
      "Epoch 40/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0065 - val_loss: 0.0038\n",
      "Epoch 41/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0063 - val_loss: 0.0037\n",
      "Epoch 42/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 43/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 44/300\n",
      "165/165 - 24s - 142ms/step - loss: 0.0059 - val_loss: 0.0033\n",
      "Epoch 45/300\n",
      "165/165 - 24s - 142ms/step - loss: 0.0058 - val_loss: 0.0033\n",
      "Epoch 46/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0056 - val_loss: 0.0032\n",
      "Epoch 47/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0055 - val_loss: 0.0031\n",
      "Epoch 48/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0054 - val_loss: 0.0030\n",
      "Epoch 49/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0053 - val_loss: 0.0029\n",
      "Epoch 50/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0052 - val_loss: 0.0029\n",
      "Epoch 51/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0051 - val_loss: 0.0028\n",
      "Epoch 52/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0050 - val_loss: 0.0027\n",
      "Epoch 53/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0049 - val_loss: 0.0027\n",
      "Epoch 54/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0048 - val_loss: 0.0026\n",
      "Epoch 55/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0047 - val_loss: 0.0025\n",
      "Epoch 56/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 57/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 58/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0044 - val_loss: 0.0023\n",
      "Epoch 59/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0044 - val_loss: 0.0023\n",
      "Epoch 60/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0043 - val_loss: 0.0022\n",
      "Epoch 61/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0042 - val_loss: 0.0022\n",
      "Epoch 62/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 63/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 64/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 65/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0039 - val_loss: 0.0020\n",
      "Epoch 66/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 67/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 68/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 69/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 70/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0036 - val_loss: 0.0018\n",
      "Epoch 71/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 72/300\n",
      "165/165 - 24s - 142ms/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 73/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0034 - val_loss: 0.0017\n",
      "Epoch 74/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0034 - val_loss: 0.0017\n",
      "Epoch 75/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 76/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 77/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 78/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 79/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 80/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 81/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0031 - val_loss: 0.0015\n",
      "Epoch 82/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 83/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 84/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 85/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 86/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 87/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 88/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 89/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 90/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 91/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 92/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 93/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 94/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 95/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 96/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 97/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 98/300\n",
      "165/165 - 24s - 146ms/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 99/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 100/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 101/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 102/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 103/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 104/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 105/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 106/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 107/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 108/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 109/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 110/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 111/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 112/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 113/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 114/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 115/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 116/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 117/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 118/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 119/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 120/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 121/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 122/300\n",
      "165/165 - 24s - 143ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 123/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 124/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 125/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 126/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 127/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 128/300\n",
      "165/165 - 24s - 145ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 129/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 130/300\n",
      "165/165 - 23s - 140ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 131/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 132/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 133/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 134/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 135/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 136/300\n",
      "165/165 - 23s - 141ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 137/300\n",
      "165/165 - 23s - 142ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 138/300\n",
      "165/165 - 23s - 140ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 139/300\n",
      "165/165 - 24s - 144ms/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 140/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 141/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 142/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 143/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 144/300\n",
      "165/165 - 25s - 149ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 145/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 146/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 147/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0016 - val_loss: 9.6847e-04\n",
      "Epoch 148/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 149/300\n",
      "165/165 - 26s - 157ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 150/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 151/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 152/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 153/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0015 - val_loss: 9.5345e-04\n",
      "Epoch 154/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 155/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 156/300\n",
      "165/165 - 26s - 156ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 157/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 158/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0015 - val_loss: 9.8803e-04\n",
      "Epoch 159/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 160/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 161/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0014 - val_loss: 9.6049e-04\n",
      "Epoch 162/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0014 - val_loss: 9.4854e-04\n",
      "Epoch 163/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0014 - val_loss: 9.4710e-04\n",
      "Epoch 164/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 165/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0014 - val_loss: 9.6221e-04\n",
      "Epoch 166/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 167/300\n",
      "165/165 - 26s - 158ms/step - loss: 0.0014 - val_loss: 9.7234e-04\n",
      "Epoch 168/300\n",
      "165/165 - 26s - 157ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 169/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0014 - val_loss: 9.7290e-04\n",
      "Epoch 170/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0014 - val_loss: 9.7010e-04\n",
      "Epoch 171/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0013 - val_loss: 9.6663e-04\n",
      "Epoch 172/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0013 - val_loss: 9.1570e-04\n",
      "Epoch 173/300\n",
      "165/165 - 25s - 150ms/step - loss: 0.0013 - val_loss: 9.5799e-04\n",
      "Epoch 174/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0013 - val_loss: 9.8929e-04\n",
      "Epoch 175/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0013 - val_loss: 9.8982e-04\n",
      "Epoch 176/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0013 - val_loss: 9.2042e-04\n",
      "Epoch 177/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0013 - val_loss: 9.0144e-04\n",
      "Epoch 178/300\n",
      "165/165 - 26s - 157ms/step - loss: 0.0013 - val_loss: 9.4607e-04\n",
      "Epoch 179/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0013 - val_loss: 9.3877e-04\n",
      "Epoch 180/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0013 - val_loss: 9.1321e-04\n",
      "Epoch 181/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0013 - val_loss: 9.3241e-04\n",
      "Epoch 182/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0013 - val_loss: 9.2943e-04\n",
      "Epoch 183/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0012 - val_loss: 9.6199e-04\n",
      "Epoch 184/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 185/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 8.7263e-04\n",
      "Epoch 186/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 9.1690e-04\n",
      "Epoch 187/300\n",
      "165/165 - 26s - 157ms/step - loss: 0.0012 - val_loss: 9.1279e-04\n",
      "Epoch 188/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 9.6124e-04\n",
      "Epoch 189/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 9.5057e-04\n",
      "Epoch 190/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0012 - val_loss: 9.0881e-04\n",
      "Epoch 191/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0012 - val_loss: 9.7383e-04\n",
      "Epoch 192/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0012 - val_loss: 9.0337e-04\n",
      "Epoch 193/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0012 - val_loss: 9.0277e-04\n",
      "Epoch 194/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0012 - val_loss: 9.7347e-04\n",
      "Epoch 195/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0012 - val_loss: 8.8914e-04\n",
      "Epoch 196/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0012 - val_loss: 8.9261e-04\n",
      "Epoch 197/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0012 - val_loss: 9.0941e-04\n",
      "Epoch 198/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0011 - val_loss: 8.9418e-04\n",
      "Epoch 199/300\n",
      "165/165 - 26s - 157ms/step - loss: 0.0011 - val_loss: 8.5894e-04\n",
      "Epoch 200/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0011 - val_loss: 8.6486e-04\n",
      "Epoch 201/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0011 - val_loss: 8.9446e-04\n",
      "Epoch 202/300\n",
      "165/165 - 26s - 156ms/step - loss: 0.0011 - val_loss: 9.8127e-04\n",
      "Epoch 203/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0011 - val_loss: 8.9087e-04\n",
      "Epoch 204/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0011 - val_loss: 8.9221e-04\n",
      "Epoch 205/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0011 - val_loss: 9.7867e-04\n",
      "Epoch 206/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0011 - val_loss: 9.3410e-04\n",
      "Epoch 207/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0011 - val_loss: 9.3489e-04\n",
      "Epoch 208/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0011 - val_loss: 8.4724e-04\n",
      "Epoch 209/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0011 - val_loss: 9.2782e-04\n",
      "Epoch 210/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0011 - val_loss: 8.9216e-04\n",
      "Epoch 211/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0011 - val_loss: 9.3013e-04\n",
      "Epoch 212/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0011 - val_loss: 8.8503e-04\n",
      "Epoch 213/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0011 - val_loss: 9.2515e-04\n",
      "Epoch 214/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0010 - val_loss: 9.2622e-04\n",
      "Epoch 215/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0010 - val_loss: 9.2559e-04\n",
      "Epoch 216/300\n",
      "165/165 - 26s - 155ms/step - loss: 0.0010 - val_loss: 8.5369e-04\n",
      "Epoch 217/300\n",
      "165/165 - 25s - 155ms/step - loss: 0.0010 - val_loss: 9.0311e-04\n",
      "Epoch 218/300\n",
      "165/165 - 25s - 152ms/step - loss: 0.0010 - val_loss: 8.7987e-04\n",
      "Epoch 219/300\n",
      "165/165 - 25s - 151ms/step - loss: 0.0010 - val_loss: 8.5988e-04\n",
      "Epoch 220/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0010 - val_loss: 8.3118e-04\n",
      "Epoch 221/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0010 - val_loss: 8.7694e-04\n",
      "Epoch 222/300\n",
      "165/165 - 25s - 154ms/step - loss: 0.0010 - val_loss: 8.4428e-04\n",
      "Epoch 223/300\n",
      "165/165 - 25s - 153ms/step - loss: 0.0010 - val_loss: 8.4474e-04\n",
      "Epoch 224/300\n",
      "165/165 - 26s - 156ms/step - loss: 9.9789e-04 - val_loss: 9.3720e-04\n",
      "Epoch 225/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.8918e-04 - val_loss: 8.5996e-04\n",
      "Epoch 226/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.8627e-04 - val_loss: 8.6206e-04\n",
      "Epoch 227/300\n",
      "165/165 - 26s - 155ms/step - loss: 9.8254e-04 - val_loss: 8.5547e-04\n",
      "Epoch 228/300\n",
      "165/165 - 25s - 154ms/step - loss: 9.7625e-04 - val_loss: 8.7231e-04\n",
      "Epoch 229/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.7069e-04 - val_loss: 7.8676e-04\n",
      "Epoch 230/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.6381e-04 - val_loss: 8.1989e-04\n",
      "Epoch 231/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.6427e-04 - val_loss: 8.9689e-04\n",
      "Epoch 232/300\n",
      "165/165 - 25s - 150ms/step - loss: 9.5827e-04 - val_loss: 8.8131e-04\n",
      "Epoch 233/300\n",
      "165/165 - 25s - 154ms/step - loss: 9.4946e-04 - val_loss: 7.6370e-04\n",
      "Epoch 234/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.4473e-04 - val_loss: 8.3523e-04\n",
      "Epoch 235/300\n",
      "165/165 - 25s - 150ms/step - loss: 9.4030e-04 - val_loss: 8.2182e-04\n",
      "Epoch 236/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.3722e-04 - val_loss: 8.7760e-04\n",
      "Epoch 237/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.3751e-04 - val_loss: 8.7848e-04\n",
      "Epoch 238/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.2997e-04 - val_loss: 9.1694e-04\n",
      "Epoch 239/300\n",
      "165/165 - 25s - 154ms/step - loss: 9.2660e-04 - val_loss: 8.6967e-04\n",
      "Epoch 240/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.2108e-04 - val_loss: 7.9603e-04\n",
      "Epoch 241/300\n",
      "165/165 - 25s - 153ms/step - loss: 9.1697e-04 - val_loss: 8.5444e-04\n",
      "Epoch 242/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.0995e-04 - val_loss: 7.9762e-04\n",
      "Epoch 243/300\n",
      "165/165 - 25s - 152ms/step - loss: 9.0430e-04 - val_loss: 7.6517e-04\n",
      "Epoch 244/300\n",
      "165/165 - 24s - 148ms/step - loss: 9.0389e-04 - val_loss: 8.3819e-04\n",
      "Epoch 245/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.9952e-04 - val_loss: 8.6690e-04\n",
      "Epoch 246/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.9338e-04 - val_loss: 8.1161e-04\n",
      "Epoch 247/300\n",
      "165/165 - 26s - 159ms/step - loss: 8.9266e-04 - val_loss: 7.8087e-04\n",
      "Epoch 248/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.8765e-04 - val_loss: 7.7076e-04\n",
      "Epoch 249/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.8387e-04 - val_loss: 8.5710e-04\n",
      "Epoch 250/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.7884e-04 - val_loss: 8.0769e-04\n",
      "Epoch 251/300\n",
      "165/165 - 25s - 150ms/step - loss: 8.7410e-04 - val_loss: 8.5784e-04\n",
      "Epoch 252/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.7214e-04 - val_loss: 9.4014e-04\n",
      "Epoch 253/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.6661e-04 - val_loss: 8.4037e-04\n",
      "Epoch 254/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.6117e-04 - val_loss: 7.4749e-04\n",
      "Epoch 255/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.6159e-04 - val_loss: 8.4744e-04\n",
      "Epoch 256/300\n",
      "165/165 - 25s - 150ms/step - loss: 8.5415e-04 - val_loss: 8.3328e-04\n",
      "Epoch 257/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.5035e-04 - val_loss: 8.6850e-04\n",
      "Epoch 258/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.4724e-04 - val_loss: 7.6123e-04\n",
      "Epoch 259/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.4481e-04 - val_loss: 8.2497e-04\n",
      "Epoch 260/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.4323e-04 - val_loss: 8.6147e-04\n",
      "Epoch 261/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.3811e-04 - val_loss: 8.7386e-04\n",
      "Epoch 262/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.3382e-04 - val_loss: 8.2379e-04\n",
      "Epoch 263/300\n",
      "165/165 - 25s - 150ms/step - loss: 8.2881e-04 - val_loss: 8.0949e-04\n",
      "Epoch 264/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.2816e-04 - val_loss: 9.0124e-04\n",
      "Epoch 265/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.2418e-04 - val_loss: 8.4438e-04\n",
      "Epoch 266/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.1991e-04 - val_loss: 8.3122e-04\n",
      "Epoch 267/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.1524e-04 - val_loss: 8.5000e-04\n",
      "Epoch 268/300\n",
      "165/165 - 26s - 156ms/step - loss: 8.1118e-04 - val_loss: 8.7643e-04\n",
      "Epoch 269/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.0977e-04 - val_loss: 9.1982e-04\n",
      "Epoch 270/300\n",
      "165/165 - 25s - 151ms/step - loss: 8.0840e-04 - val_loss: 8.7971e-04\n",
      "Epoch 271/300\n",
      "165/165 - 25s - 153ms/step - loss: 8.0244e-04 - val_loss: 7.8432e-04\n",
      "Epoch 272/300\n",
      "165/165 - 25s - 152ms/step - loss: 8.0173e-04 - val_loss: 8.3760e-04\n",
      "Epoch 273/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.9752e-04 - val_loss: 8.0895e-04\n",
      "Epoch 274/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.9372e-04 - val_loss: 8.2391e-04\n",
      "Epoch 275/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.8957e-04 - val_loss: 7.3513e-04\n",
      "Epoch 276/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.8667e-04 - val_loss: 7.9609e-04\n",
      "Epoch 277/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.8272e-04 - val_loss: 7.9116e-04\n",
      "Epoch 278/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.7981e-04 - val_loss: 7.5678e-04\n",
      "Epoch 279/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.7924e-04 - val_loss: 8.7404e-04\n",
      "Epoch 280/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.7372e-04 - val_loss: 8.1324e-04\n",
      "Epoch 281/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.7801e-04 - val_loss: 8.5029e-04\n",
      "Epoch 282/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.6889e-04 - val_loss: 7.6933e-04\n",
      "Epoch 283/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.6601e-04 - val_loss: 9.2770e-04\n",
      "Epoch 284/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.6267e-04 - val_loss: 7.5745e-04\n",
      "Epoch 285/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.5773e-04 - val_loss: 7.6638e-04\n",
      "Epoch 286/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.5774e-04 - val_loss: 7.5414e-04\n",
      "Epoch 287/300\n",
      "165/165 - 25s - 153ms/step - loss: 7.5513e-04 - val_loss: 7.5408e-04\n",
      "Epoch 288/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.5245e-04 - val_loss: 7.6938e-04\n",
      "Epoch 289/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.4984e-04 - val_loss: 8.3407e-04\n",
      "Epoch 290/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.4618e-04 - val_loss: 9.0911e-04\n",
      "Epoch 291/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.4149e-04 - val_loss: 8.9412e-04\n",
      "Epoch 292/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.4066e-04 - val_loss: 8.2185e-04\n",
      "Epoch 293/300\n",
      "165/165 - 25s - 154ms/step - loss: 7.3882e-04 - val_loss: 7.6684e-04\n",
      "Epoch 294/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.3382e-04 - val_loss: 7.6058e-04\n",
      "Epoch 295/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.3286e-04 - val_loss: 7.8923e-04\n",
      "Epoch 296/300\n",
      "165/165 - 25s - 153ms/step - loss: 7.2696e-04 - val_loss: 7.9061e-04\n",
      "Epoch 297/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.2845e-04 - val_loss: 7.8899e-04\n",
      "Epoch 298/300\n",
      "165/165 - 25s - 152ms/step - loss: 7.2585e-04 - val_loss: 8.5751e-04\n",
      "Epoch 299/300\n",
      "165/165 - 25s - 150ms/step - loss: 7.1947e-04 - val_loss: 8.5237e-04\n",
      "Epoch 300/300\n",
      "165/165 - 25s - 151ms/step - loss: 7.1624e-04 - val_loss: 7.7788e-04\n",
      "Training time: 7351.033 seconds\n",
      "Model saved as ./DH2-1024.weights.h5\n",
      "Validation Loss: 0.0008\n",
      "The final RMSE = 0.0287, The final MAE = 0.0223\n",
      "Test time: 2.936 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import data_read_2\n",
    "\n",
    "# ---------------------- #\n",
    "#       THAM SỐ        #\n",
    "# ---------------------- #\n",
    "seq_length = 1024\n",
    "line_num = 1000\n",
    "\n",
    "# ---------------------- #\n",
    "#  ĐỌC VÀ XỬ LÝ DỮ LIỆU  #\n",
    "# ---------------------- #\n",
    "# Đọc dữ liệu (giả sử data_read_2 có các hàm data_read và data_embedding)\n",
    "X_data, y_data, Label = data_read_2.data_read(seq_length, line_num)\n",
    "X_data, y_data = data_read_2.data_embedding(X_data, y_data, seq_length)\n",
    "print(\"Total data volume: {}\".format(len(X_data)))\n",
    "\n",
    "# Trộn dữ liệu: ghép X_data, y_data và Label lại với nhau rồi random.shuffle\n",
    "Data = list(zip(X_data, y_data, Label))\n",
    "random.shuffle(Data)\n",
    "X_data, y_data, Label = zip(*Data)\n",
    "X_data, y_data, Label = np.array(X_data), np.array(y_data), np.array(Label)\n",
    "\n",
    "# Tách dữ liệu theo tỷ lệ Train 90%, Val 5%, Test 5%\n",
    "num_total = len(X_data)\n",
    "train_end = int(num_total * 0.9) - 1\n",
    "val_end = int(num_total * 0.95) - 1\n",
    "\n",
    "X_train = X_data[:train_end]\n",
    "y_train = y_data[:train_end]\n",
    "\n",
    "X_val = X_data[train_end:val_end]\n",
    "y_val = y_data[train_end:val_end]\n",
    "\n",
    "X_test = X_data[val_end:num_total-1]\n",
    "y_test = y_data[val_end:num_total-1]\n",
    "Label_test = Label[val_end:num_total-1]\n",
    "\n",
    "print(\"Train data volume: {}, Val data volume: {}, Test data volume: {}\"\n",
    "      .format(len(X_train), len(X_val), len(X_test)))\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#    THAM SỐ HYPERPARAMS #\n",
    "# ---------------------- #\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "hidden_units = seq_length // 8\n",
    "maxlen = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "num_epochs = 300\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "# ---------------------- #\n",
    "#  CHUẨN BỊ DỮ LIỆU    #\n",
    "# ---------------------- #\n",
    "# Ép kiểu và chuyển đổi dữ liệu sang numpy array kiểu float32\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "X_val   = np.array(X_val, dtype=np.float32)\n",
    "y_val   = np.array(y_val, dtype=np.float32)\n",
    "X_test  = np.array(X_test, dtype=np.float32)\n",
    "y_test  = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "# Tạo tf.data.Dataset cho train, validation và test\n",
    "train_inputs = (X_train, y_train)\n",
    "train_targets = y_train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(batch_size)\n",
    "\n",
    "val_inputs = (X_val, y_val)\n",
    "val_targets = y_val\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_targets)).batch(batch_size)\n",
    "\n",
    "test_inputs = (X_test, y_test)\n",
    "test_targets = y_test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_targets)).batch(32)\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#    ĐỊNH NGHĨA MODEL   #\n",
    "# ---------------------- #\n",
    "\n",
    "# MultiHeadAttention custom layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate, causality=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.linear_Q = tf.keras.layers.Dense(d_model)\n",
    "        self.linear_K = tf.keras.layers.Dense(d_model)\n",
    "        self.linear_V = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.causality = causality\n",
    "\n",
    "    def call(self, queries, keys, values, training):\n",
    "        # queries, keys, values: shape (batch, seq_len, d_model)\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        Q = self.linear_Q(queries)  # (batch, seq_len, d_model)\n",
    "        K = self.linear_K(keys)\n",
    "        V = self.linear_V(values)\n",
    "        \n",
    "        # Tách ra theo số head: chuyển về shape (batch, num_heads, seq_len, d_k)\n",
    "        def split_heads(x):\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "        \n",
    "        # Tính attention scores với scaled dot-product\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_k, tf.float32))\n",
    "        if self.causality:\n",
    "            # Tạo mask causal: chỉ cho phép truy cập thông tin quá khứ\n",
    "            seq_len = tf.shape(scores)[-1]\n",
    "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            mask = tf.cast(mask, tf.bool)\n",
    "            scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        attn = self.dropout(attn, training=training)\n",
    "        output = tf.matmul(attn, V)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Nối lại các head\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch, seq_len, num_heads, d_k)\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Kết hợp residual connection và layer normalization\n",
    "        output = self.layer_norm(output + queries)\n",
    "        return output\n",
    "\n",
    "# FeedForward layer\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(4 * d_model)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        residual = x\n",
    "        x = self.linear1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x\n",
    "\n",
    "# Encoder block\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=False)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        x = self.mha(x, x, x, training=training)\n",
    "        x = self.ffn(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Decoder block\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=True)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads, dropout_rate, causality=False)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training):\n",
    "        x = self.mha1(x, x, x, training=training)\n",
    "        x = self.mha2(x, enc_output, enc_output, training=training)\n",
    "        x = self.ffn(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Linear layer cuối cùng\n",
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        self.output_scale = self.add_weight(name=\"output_scale\", shape=(), initializer=tf.keras.initializers.Ones())\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.linear(x) * self.output_scale\n",
    "\n",
    "# Mô hình Transformer chính\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_rate, lambda_loss_amount):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layers = [EncoderBlock(d_model, num_heads, dropout_rate) for _ in range(num_encoder_layers)]\n",
    "        self.decoder_layers = [DecoderBlock(d_model, num_heads, dropout_rate) for _ in range(num_decoder_layers)]\n",
    "        self.linear_layer = LinearLayer(d_model)\n",
    "        self.lambda_loss_amount = lambda_loss_amount\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs là một tuple chứa (encoder_input, decoder_input)\n",
    "        encoder_input, decoder_input = inputs\n",
    "        for enc in self.encoder_layers:\n",
    "            encoder_input = enc(encoder_input, training=training)\n",
    "        enc_output = encoder_input\n",
    "        for dec in self.decoder_layers:\n",
    "            decoder_input = dec(decoder_input, enc_output, training=training)\n",
    "        pred = self.linear_layer(decoder_input)\n",
    "        return pred\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#    KHỞI TẠO MODEL    #\n",
    "# ---------------------- #\n",
    "model = TransformerModel(d_model=hidden_units, num_heads=num_heads,\n",
    "                         num_encoder_layers=num_encoder_layers,\n",
    "                         num_decoder_layers=num_decoder_layers,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                         lambda_loss_amount=lambda_loss_amount)\n",
    "\n",
    "# Biên dịch model với optimizer và loss (MSE)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# ---------------------- #\n",
    "#       HUẤN LUYỆN      #\n",
    "# ---------------------- #\n",
    "print(\"Training...\\n\")\n",
    "start_time = time.time()\n",
    "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=2)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Lưu model (chỉ lưu weights)\n",
    "model_path = f'./DH2-{seq_length}.weights.h5'\n",
    "model.save_weights(model_path)\n",
    "print(f\"Model saved as {model_path}\")\n",
    "\n",
    "# Đánh giá trên tập validation\n",
    "val_loss = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Tải lại model đã lưu để demo\n",
    "model_loaded = TransformerModel(d_model=hidden_units, num_heads=num_heads,\n",
    "                                num_encoder_layers=num_encoder_layers,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                lambda_loss_amount=lambda_loss_amount)\n",
    "# Đảm bảo xây dựng model bằng cách chạy một batch mẫu:\n",
    "dummy_encoder = tf.random.uniform((1, X_train.shape[1], hidden_units))\n",
    "dummy_decoder = tf.random.uniform((1, y_train.shape[1], hidden_units))\n",
    "_ = model_loaded((dummy_encoder, dummy_decoder))\n",
    "model_loaded.load_weights(model_path)\n",
    "\n",
    "# ---------------------- #\n",
    "#         KIỂM TRA      #\n",
    "# ---------------------- #\n",
    "test_time_start = time.time()\n",
    "all_preds = []\n",
    "for (enc_input, dec_input), target in test_dataset:\n",
    "    preds = model_loaded((enc_input, dec_input), training=False)\n",
    "    all_preds.append(preds)\n",
    "all_preds = tf.concat(all_preds, axis=0)\n",
    "test_time_end = time.time()\n",
    "test_time = test_time_end - test_time_start\n",
    "\n",
    "\n",
    "# Tính RMSE và MAE\n",
    "mse = tf.keras.losses.MeanSquaredError()(y_test, all_preds)\n",
    "rmse = tf.sqrt(mse).numpy()\n",
    "mae = tf.keras.losses.MeanAbsoluteError()(y_test, all_preds).numpy()\n",
    "\n",
    "print(f\"The final RMSE = {rmse:.4f}, The final MAE = {mae:.4f}\")\n",
    "print(f\"Test time: {test_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại model đã lưu để demo\n",
    "model_loaded = TransformerModel(d_model=hidden_units, num_heads=num_heads,\n",
    "                                num_encoder_layers=num_encoder_layers,\n",
    "                                num_decoder_layers=num_decoder_layers,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                lambda_loss_amount=lambda_loss_amount)\n",
    "# Đảm bảo xây dựng model bằng cách chạy một batch mẫu:\n",
    "dummy_encoder = tf.random.uniform((1, X_train.shape[1], hidden_units))\n",
    "dummy_decoder = tf.random.uniform((1, y_train.shape[1], hidden_units))\n",
    "_ = model_loaded((dummy_encoder, dummy_decoder))\n",
    "model_loaded.load_weights(model_path)\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "for (enc_input, dec_input), target in test_dataset:\n",
    "    preds = model_loaded((enc_input, dec_input), training=False)\n",
    "    all_preds.append(preds)\n",
    "all_preds = tf.concat(all_preds, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1172, 8, 128), dtype=float32, numpy=\n",
       "array([[[ 0.05106504, -0.13922325, -0.06660241, ...,  0.83283526,\n",
       "         -0.12355666,  0.28127617],\n",
       "        [ 0.67505556, -0.17277564,  0.38824514, ..., -0.03918505,\n",
       "         -0.51959044, -0.2400989 ],\n",
       "        [-0.2388668 , -0.41212264, -0.32615992, ..., -0.51913464,\n",
       "         -0.5511497 , -0.26644287],\n",
       "        ...,\n",
       "        [-0.24811828, -0.41918743, -0.22707579, ..., -0.27531305,\n",
       "         -0.47784632,  0.01447508],\n",
       "        [-0.250187  , -0.08815622, -0.09252882, ...,  0.013631  ,\n",
       "          0.14644887,  0.02065367],\n",
       "        [ 0.19374362, -0.15211338,  0.08849527, ..., -0.24752322,\n",
       "         -0.3602416 ,  0.05745627]],\n",
       "\n",
       "       [[ 0.16510765,  0.12859018,  0.10617515, ...,  0.34806132,\n",
       "          0.34531277,  0.43499717],\n",
       "        [ 0.46757665,  0.45074418,  0.44492045, ..., -0.1327413 ,\n",
       "         -0.09381447, -0.1485674 ],\n",
       "        [-0.1388901 , -0.2078002 , -0.3584356 , ..., -0.25136536,\n",
       "         -0.09517656, -0.13073829],\n",
       "        ...,\n",
       "        [-0.14329277, -0.17080048, -0.35004392, ..., -0.32059717,\n",
       "         -0.30652523, -0.26903945],\n",
       "        [-0.27886465, -0.11204652, -0.17234296, ...,  0.05580218,\n",
       "          0.00698032, -0.01274224],\n",
       "        [-0.14172772, -0.04673547, -0.09186163, ..., -0.29725444,\n",
       "         -0.29687682, -0.20203392]],\n",
       "\n",
       "       [[-0.30239853, -0.553165  , -0.47660547, ..., -0.05882667,\n",
       "         -0.13661312,  0.0333377 ],\n",
       "        [-0.10901849, -0.10436633,  0.15795304, ...,  0.10594099,\n",
       "         -0.20893839, -0.14556725],\n",
       "        [ 0.21367413, -0.141766  , -0.61421067, ..., -0.24752249,\n",
       "         -0.33887485, -0.5157663 ],\n",
       "        ...,\n",
       "        [ 0.0053579 ,  0.12025499,  0.03067011, ..., -0.07178423,\n",
       "         -0.38054696, -0.23853305],\n",
       "        [-0.07641274, -0.3467725 , -0.2357263 , ...,  0.27361175,\n",
       "          0.12016303,  0.3297723 ],\n",
       "        [ 0.311857  ,  0.32244697,  0.12364706, ...,  0.20862763,\n",
       "          0.21004991,  0.27702355]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.2227611 ,  0.11558469,  0.05693218, ..., -0.35189095,\n",
       "         -0.05052188, -0.266607  ],\n",
       "        [ 0.01719567, -0.3324407 , -0.462715  , ...,  0.07593774,\n",
       "          0.3254255 , -0.14530548],\n",
       "        [ 0.40831482,  0.24881229,  0.03768427, ..., -0.20697744,\n",
       "         -0.18514766, -0.20170437],\n",
       "        ...,\n",
       "        [-0.27248448, -0.17912866, -0.21684016, ..., -0.31097913,\n",
       "         -0.16379352, -0.20328501],\n",
       "        [-0.23987131, -0.12873633, -0.51827836, ..., -0.03449207,\n",
       "         -0.36122787, -0.15933539],\n",
       "        [-0.3308248 , -0.00116048, -0.35729206, ...,  0.10123136,\n",
       "          0.00222643,  0.06985956]],\n",
       "\n",
       "       [[ 0.12332065,  0.31851143,  0.02965218, ..., -0.16967435,\n",
       "         -0.08627081, -0.14514042],\n",
       "        [-0.04658814, -0.08828766,  0.00847312, ..., -0.1965443 ,\n",
       "         -0.48591805, -0.60580516],\n",
       "        [-0.22753294, -0.84051275, -0.44322243, ...,  0.00976156,\n",
       "         -0.07483013, -0.15416028],\n",
       "        ...,\n",
       "        [-0.41797498, -0.3963452 , -0.4269745 , ..., -0.12514025,\n",
       "         -0.06779447,  0.00813592],\n",
       "        [-0.11372118,  0.08764219,  0.11728194, ...,  0.27497342,\n",
       "         -0.12530482, -0.10403237],\n",
       "        [ 0.08462168, -0.19632542, -0.02176075, ..., -0.0888485 ,\n",
       "         -0.08977809, -0.26002985]],\n",
       "\n",
       "       [[ 0.47474912,  0.41983205,  0.25020978, ...,  0.3944229 ,\n",
       "         -0.35111576,  0.11663084],\n",
       "        [-0.5669248 ,  0.0921374 ,  0.11708426, ..., -0.57617015,\n",
       "         -0.41296875, -0.34374827],\n",
       "        [-0.1241942 ,  0.10729048, -0.20156649, ..., -0.03815892,\n",
       "          0.5697976 ,  0.34181818],\n",
       "        ...,\n",
       "        [-0.20378807, -0.2202162 , -0.02634895, ...,  0.2548103 ,\n",
       "         -0.25605702, -0.13327473],\n",
       "        [ 0.12872319, -0.31630224,  0.25873145, ..., -0.26442137,\n",
       "         -0.34907818, -0.24674366],\n",
       "        [-0.38406134, -0.5206556 , -0.27147317, ..., -0.387356  ,\n",
       "         -0.34867072, -0.229631  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [2],\n",
       "       [5],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Label_preds.csv đã được lưu thành công.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chuyển Label_test thành DataFrame\n",
    "df_label_test = pd.DataFrame(Label_test)\n",
    "\n",
    "# Lưu DataFrame thành file CSV\n",
    "df_label_test.to_csv('Label_preds.csv', index=False)\n",
    "\n",
    "print(\"File Label_preds.csv đã được lưu thành công.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
